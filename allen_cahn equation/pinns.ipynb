{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvbUQFgyzwUV",
        "outputId": "b67ad3aa-d374-4204-ff00-17697fd99ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/pinn-main; to attempt to forcibly remount, call drive.mount(\"/content/pinn-main\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/pinn-main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1H1ywRoz1LP",
        "outputId": "f142a99c-1d66-40aa-bfe3-7ee6730ec038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pinn-main/MyDrive/pinns-main\n"
          ]
        }
      ],
      "source": [
        "cd /content/pinn-main/MyDrive/pinns-main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TUqIB_wYcIwv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Сохранение результатов"
      ],
      "metadata": {
        "id": "2nzTi77IxjK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qU"
      ],
      "metadata": {
        "id": "LDJS3YsOxo1p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "# import random\n",
        "# import math"
      ],
      "metadata": {
        "id": "wJjKQsYTx4_i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.login()"
      ],
      "metadata": {
        "id": "56O4QCDlx8D8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(\n",
        "#     # Set the project where this run will be logged\n",
        "#     project=\"Allen-cahn\",\n",
        "#     # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "#     name=f\"переделанный\",\n",
        "#     # Track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"epochs\": 100000,\n",
        "#     })"
      ],
      "metadata": {
        "id": "fHGHXxjPy3gt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Импортированный блок\n"
      ],
      "metadata": {
        "id": "QbNrwbQNm9iV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "          data.py\n",
        "          "
      ],
      "metadata": {
        "id": "haT2SyaunFhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def ac_generator(num_t, num_x, typ='train'):\n",
        "    N_f = num_t*num_x\n",
        "    t = np.linspace(0, 1, num_t).reshape(-1,1) # T x 1\n",
        "    x = np.linspace(-1, 1, num_x).reshape(-1,1) # N x 1\n",
        "    T = t.shape[0]\n",
        "    N = x.shape[0]\n",
        "    T_star = np.tile(t, (1, N)).T  # N x T\n",
        "    X_star = np.tile(x, (1, T))  # N x T\n",
        "\n",
        "    # Initial condition and boundary condition\n",
        "    u = np.zeros((N, T))  # N x T\n",
        "    u[:,0:1] = (x**2)*np.cos(np.pi*x)\n",
        "    u[0,:] = -np.ones(T)\n",
        "    u[-1,:] = u[0,:]\n",
        "\n",
        "    t_data = T_star.flatten()[:, None]\n",
        "    x_data = X_star.flatten()[:, None]\n",
        "    u_data = u.flatten()[:, None]\n",
        "\n",
        "    t_data_f = t_data.copy()\n",
        "    x_data_f = x_data.copy()\n",
        "\n",
        "    if typ == 'train':\n",
        "        idx = np.random.choice(np.where((x_data == -1) | (x_data == 1))[0], num_t)\n",
        "        t_data = t_data[idx]\n",
        "        x_data = x_data[idx]\n",
        "        u_data = u_data[idx]\n",
        "\n",
        "        init_idx = np.random.choice(N-1, num_x-4, replace=False) + 1\n",
        "        t_data = np.concatenate([t_data, np.ones((2,1)), np.zeros((num_x-4,1))], axis=0)\n",
        "        x_data = np.concatenate([x_data, np.array([[-1], [1]]), x[init_idx]], axis=0)\n",
        "        u_data = np.concatenate([u_data, -np.ones((2,1)), u[init_idx,0:1]], axis=0)\n",
        "\n",
        "        return t_data, x_data, u_data, t_data_f, x_data_f\n",
        "\n",
        "    else:\n",
        "        return t_data_f, x_data_f\n"
      ],
      "metadata": {
        "id": "42kiubIdnDGh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    model.py"
      ],
      "metadata": {
        "id": "StXL8Ef5nNSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class LinearBlock(nn.Module):\n",
        "\n",
        "#     def __init__(self, in_nodes, out_nodes):\n",
        "#         super(LinearBlock, self).__init__()\n",
        "#         self.layer = nn.utils.weight_norm(nn.Linear(in_nodes, out_nodes), dim = 0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.layer(x)\n",
        "#         x = torch.tanh(x)\n",
        "#         return x\n",
        "\n",
        "# class PINN(nn.Module):\n",
        "\n",
        "#     def __init__(self, layer_list):\n",
        "#         super().__init__()\n",
        "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#         self.input_layer = nn.utils.weight_norm(nn.Linear(layer_list[0], layer_list[1]), dim = 0)\n",
        "#         self.hidden_layers = self._make_layer(layer_list[1:-1])\n",
        "#         self.output_layer = nn.Linear(layer_list[-2], layer_list[-1])\n",
        "\n",
        "#     def _make_layer(self, layer_list):\n",
        "#         layers = []\n",
        "#         for i in range(len(layer_list) - 1):\n",
        "#             block = LinearBlock(layer_list[i], layer_list[i + 1])\n",
        "#             layers.append(block)\n",
        "#         return nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.input_layer(x)\n",
        "#         x = torch.tanh(x)\n",
        "#         x = self.hidden_layers(x)\n",
        "#         x = self.output_layer(x)\n",
        "#         return x\n",
        "\n",
        "# def pinn(layer_list):\n",
        "#     model = PINN(layer_list)\n",
        "#     # model.apply(weights_init)\n",
        "#     return model\n",
        "\n",
        "# def weights_init(m):\n",
        "#     if isinstance(m, nn.Linear):\n",
        "#         torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "      super().__init__()\n",
        "      device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      print(2)\n",
        "      self.layers_stack = nn.Sequential(\n",
        "            nn.utils.weight_norm(nn.Linear(2, hidden_size), dim = 0),\n",
        "            nn.Tanh(),\n",
        "            nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size), dim=0),\n",
        "            nn.Tanh(),\n",
        "            nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size), dim=0),\n",
        "            nn.Tanh(),\n",
        "            nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size), dim=0),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "      return self.layers_stack(x)\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "def pinn(hidden_size):\n",
        "    model = PINN(hidden_size)\n",
        "    model.apply(weights_init)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Q06U04mcnQ7Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "      utils.py"
      ],
      "metadata": {
        "id": "PviapM9-nUCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def fwd_gradients(obj, x):\n",
        "    dummy = torch.ones_like(obj)\n",
        "    derivative = torch.autograd.grad(obj, x, dummy, create_graph= True)[0]\n",
        "    return derivative\n",
        "\n",
        "\n",
        "def ac_equation(u, tx):\n",
        "    # u_tx = fwd_gradients(u, tx)\n",
        "    u_tx = torch.autograd.grad(u, tx, torch.ones_like(u), create_graph= True)[0]\n",
        "    u_t = u_tx[:, 0:1]\n",
        "    u_x = u_tx[:, 1:2]\n",
        "    # u_xx = fwd_gradients(u_x, tx)[:, 1:2]\n",
        "    u_xx = torch.autograd.grad(u_x, tx, torch.ones_like(u_x), create_graph= True)[0][:, 1:2]\n",
        "    e = u_t -0.0001*u_xx + 5*u**3 - 5*u\n",
        "    return e\n",
        "\n",
        "def resplot(x, t, t_data, x_data, Exact, u_pred):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(x, Exact[:,0],'-')\n",
        "    plt.plot(x, u_pred[:,0],'--')\n",
        "    plt.legend(['Reference', 'Prediction'])\n",
        "    plt.title(\"Initial condition ($t=0$)\")\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    t_step = int(0.25*len(t))\n",
        "    plt.plot(x, Exact[:,t_step],'-')\n",
        "    plt.plot(x, u_pred[:,t_step],'--')\n",
        "    plt.legend(['Reference', 'Prediction'])\n",
        "    plt.title(\"$t=0.25$\")\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    t_step = int(0.5*len(t))\n",
        "    plt.plot(x, Exact[:,t_step],'-')\n",
        "    plt.plot(x, u_pred[:,t_step],'--')\n",
        "    plt.legend(['Reference', 'Prediction'])\n",
        "    plt.title(\"$t=0.5$\")\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    t_step = int(0.99*len(t))\n",
        "    plt.plot(x, Exact[:,t_step],'-')\n",
        "    plt.plot(x, u_pred[:,t_step],'--')\n",
        "    plt.legend(['Reference', 'Prediction'])\n",
        "    plt.title(\"$t=0.99$\")\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "MKyknuOjnXMS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Default Setting\n",
        "\n",
        "1. Domain: 100 x 256 ($x \\in [-1,1]$ and $t \\in [0,1]$)\n",
        "\n",
        "2. Collocation points: $N_{ic}=256$ and $N_{f}=25600$\n",
        "\n",
        "3. Optimizer: Adam with the learning rate of $10^{-3}$\n"
      ],
      "metadata": {
        "id": "-8zuaZ10oqNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(44)\n",
        "np.random.seed(44)\n",
        "\n",
        "num_t = 100\n",
        "num_x = 256\n",
        "num_epochs = 100000\n",
        "num_hidden = 4\n",
        "num_nodes = 128\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "5ZHYHnJmcWGI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a partial differential equation\n",
        "eq = 'ac' # or 'bg'"
      ],
      "metadata": {
        "id": "itRwAqd_fofj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Train Data"
      ],
      "metadata": {
        "id": "l_bowpD0ohX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Operation mode: \", device)"
      ],
      "metadata": {
        "id": "Bhdc83ZwoYGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623edfd1-9ff7-4a49-9252-d69a588864bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation mode:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if eq == 'ac':\n",
        "    t_data, x_data, u_data, t_data_f, x_data_f = ac_generator(num_t, num_x)\n",
        "else:\n",
        "    print(\"There exists no the equation.\")\n",
        "    exit(0)"
      ],
      "metadata": {
        "id": "0tyofUB6LGX0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables = torch.FloatTensor(np.concatenate((t_data, x_data), 1)).to(device)\n",
        "variables_f = torch.FloatTensor(np.concatenate((t_data_f, x_data_f), 1)).to(device)\n",
        "variables_f.requires_grad = True\n",
        "u_data = torch.FloatTensor(u_data).to(device)"
      ],
      "metadata": {
        "id": "SGY-Wk7XZ4z8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Neural Network"
      ],
      "metadata": {
        "id": "Hw-Sg8CAokST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layer_list = [2] + num_hidden * [num_nodes] + [1]\n",
        "# pinn = pinn(layer_list).to(device)\n",
        "pinn = pinn(num_nodes).to(device)"
      ],
      "metadata": {
        "id": "eQWRmH9TgOTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3320941c-b701-4e3f-8f62-9f5eec5d6620"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training Session"
      ],
      "metadata": {
        "id": "nm2ZJfPPdT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(pinn.parameters(), betas=(0.999,0.999), lr=lr)\n",
        "\n",
        "loss_graph = []\n",
        "ls = 1e-3\n",
        "bep = 0"
      ],
      "metadata": {
        "id": "To8vJn-SmobB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateL2():\n",
        "  t = np.linspace(0, 1, 201).reshape(-1,1) # T x 1\n",
        "  x = np.linspace(-1, 1, 513)[:-1].reshape(-1,1) # N x 1\n",
        "  T = t.shape[0]\n",
        "  N = x.shape[0]\n",
        "  T_star = np.tile(t, (1, N)).T  # N x T\n",
        "  X_star = np.tile(x, (1, T))  # N x T\n",
        "  t_test = T_star.flatten()[:, None]\n",
        "  x_test = X_star.flatten()[:, None]\n",
        "\n",
        "  test_variables = torch.FloatTensor(np.concatenate((t_test, x_test), 1)).to(device)\n",
        "  with torch.no_grad():\n",
        "      u_pred = pinn(test_variables)\n",
        "  u_pred = u_pred.cpu().numpy().reshape(N,T)\n",
        "\n",
        "  # reference data\n",
        "  data = scipy.io.loadmat('./data/AC.mat')\n",
        "  Exact = np.real(data['uu'])\n",
        "  err = u_pred-Exact\n",
        "  err = np.linalg.norm(err,2)/np.linalg.norm(Exact,2)\n",
        "  # print(f\"L2 Relative Error: {err}\")\n",
        "  return err"
      ],
      "metadata": {
        "id": "8WH9dahWueiS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in tqdm(range(num_epochs)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Full batch\n",
        "        u_hat = pinn(variables)\n",
        "        u_hat_f = pinn(variables_f)\n",
        "\n",
        "        loss_f = torch.mean(ac_equation(u_hat_f, variables_f) ** 2)\n",
        "\n",
        "        loss_u = torch.mean((u_hat - u_data) ** 2)\n",
        "        loss = loss_f + loss_u\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        l = loss.item()\n",
        "        loss_graph.append(l)\n",
        "        if l < ls:\n",
        "            ls = l\n",
        "            bep = ep\n",
        "            torch.save(pinn.state_dict(), './'+eq+'_1d.pth')\n",
        "\n",
        "        if ep % 100 == 0:\n",
        "            print(f\"Train loss: {l}\")\n",
        "            # wandb.log({\"epoche\": ep, \"loss\": loss})\n",
        "        if ep % 500 == 0:\n",
        "            l2 = calculateL2()\n",
        "            # wandb.log({\"epoche\": ep, \"L2\": l2})\n",
        "            print(f\"L2 Relative Error: {l2}\")\n",
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "j95KJ_DKdbTr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "01ff87a3-9981-4aaa-8cc0-2711af6c288a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5459659099578857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/100000 [00:09<253:30:20,  9.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Relative Error: 1.0401084765368944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 50/100000 [01:30<50:28:57,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-73da40810c04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[Best][Epoch: {bep}] Train loss: {ls}\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_graph)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mDkB-irbdtXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Inference Session"
      ],
      "metadata": {
        "id": "rU54g9ZAd2ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinn.load_state_dict(torch.load('./'+eq+'_1d.pth'))"
      ],
      "metadata": {
        "id": "R8t3Xbond6Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if eq == 'ac':\n",
        "    t = np.linspace(0, 1, 201).reshape(-1,1) # T x 1\n",
        "    x = np.linspace(-1, 1, 513)[:-1].reshape(-1,1) # N x 1\n",
        "    T = t.shape[0]\n",
        "    N = x.shape[0]\n",
        "    T_star = np.tile(t, (1, N)).T  # N x T\n",
        "    X_star = np.tile(x, (1, T))  # N x T\n",
        "    t_test = T_star.flatten()[:, None]\n",
        "    x_test = X_star.flatten()[:, None]\n",
        "\n",
        "    test_variables = torch.FloatTensor(np.concatenate((t_test, x_test), 1)).to(device)\n",
        "    with torch.no_grad():\n",
        "        u_pred = pinn(test_variables)\n",
        "    u_pred = u_pred.cpu().numpy().reshape(N,T)\n",
        "\n",
        "    # reference data\n",
        "    data = scipy.io.loadmat('./data/AC.mat')\n",
        "    Exact = np.real(data['uu'])\n",
        "    err = u_pred-Exact\n",
        "\n",
        "err = np.linalg.norm(err,2)/np.linalg.norm(Exact,2)\n",
        "print(f\"L2 Relative Error: {err}\")"
      ],
      "metadata": {
        "id": "9i2DMxVleAt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Result Figures"
      ],
      "metadata": {
        "id": "A0e_TOeJeJ-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resplot(x, t, t_data, x_data, Exact, u_pred)"
      ],
      "metadata": {
        "id": "w5BtQDqxeIY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(u_pred, interpolation='nearest', cmap='jet',\n",
        "            extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "            origin='lower', aspect='auto')\n",
        "plt.clim(-1, 1)\n",
        "plt.ylim(-1,1)\n",
        "plt.xlim(0,1)\n",
        "plt.scatter(t_data, x_data)\n",
        "plt.xlabel('t')\n",
        "plt.ylabel('x')\n",
        "plt.title('u(t,x)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mYNHCFMGeiMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzxMaOZkghmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}