{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvbUQFgyzwUV",
    "outputId": "264b332e-1fed-410d-b3cb-aebbc58664f9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/pinn-main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1H1ywRoz1LP",
    "outputId": "0a803244-b50d-4c8f-c2af-6609ceffee7f"
   },
   "outputs": [],
   "source": [
    "# cd /content/pinn-main/MyDrive/pinns-main/modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TUqIB_wYcIwv"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n-Gqlyv1uUUd"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb -qU\n",
    "# !pip install ml_collections\n",
    "import modulus\n",
    "import data_generator as dg\n",
    "import default\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7R7sneLLjIDi"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "np.random.seed(44)\n",
    "torch.cuda.manual_seed(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PviapM9-nUCm"
   },
   "source": [
    "      utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MKyknuOjnXMS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def ac_equation(u, tx):\n",
    "    u_tx = torch.autograd.grad(u, tx, torch.ones_like(u), create_graph= True)[0]\n",
    "    u_t = u_tx[:, 0:1]\n",
    "    u_x = u_tx[:, 1:2]\n",
    "    u_xx = torch.autograd.grad(u_x, tx, torch.ones_like(u_x), create_graph= True)[0][:, 1:2]\n",
    "    e = u_t -0.0001*u_xx + 5*u**3 - 5*u\n",
    "    return e\n",
    "\n",
    "def resplot(x, t, t_data, x_data, Exact, u_pred):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(x, Exact[:,0],'-')\n",
    "    plt.plot(x, u_pred[:,0],'--')\n",
    "    plt.legend(['Reference', 'Prediction'])\n",
    "    plt.title(\"Initial condition ($t=0$)\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    t_step = int(0.25*len(t))\n",
    "    plt.plot(x, Exact[:,t_step],'-')\n",
    "    plt.plot(x, u_pred[:,t_step],'--')\n",
    "    plt.legend(['Reference', 'Prediction'])\n",
    "    plt.title(\"$t=0.25$\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    t_step = int(0.5*len(t))\n",
    "    plt.plot(x, Exact[:,t_step],'-')\n",
    "    plt.plot(x, u_pred[:,t_step],'--')\n",
    "    plt.legend(['Reference', 'Prediction'])\n",
    "    plt.title(\"$t=0.5$\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    t_step = int(0.99*len(t))\n",
    "    plt.plot(x, Exact[:,t_step],'-')\n",
    "    plt.plot(x, u_pred[:,t_step],'--')\n",
    "    plt.legend(['Reference', 'Prediction'])\n",
    "    plt.title(\"$t=0.99$\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wMgtwZuu16Kk"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TrainClass:\n",
    "    def __init__(self, cfg, wandbFlag=False):\n",
    "        # Конфигурация\n",
    "        self.num_t = cfg.num_t\n",
    "        self.num_x = cfg.num_x\n",
    "        self.num_epochs = cfg.epochs\n",
    "        self.num_hidden_layers = 4\n",
    "        self.num_nodes = cfg.hidden_count\n",
    "        self.learning_rate = cfg.lr\n",
    "        self.data_path = cfg.data_path\n",
    "        self.wandbFlag = wandbFlag\n",
    "\n",
    "        #Подключение отслеживания с помощью wandb\n",
    "        if self.wandbFlag:\n",
    "          self.__wandbConnect(cfg)\n",
    "\n",
    "        # Устройство\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Operation mode: \", self.device)\n",
    "\n",
    "        # Данные\n",
    "        self.__createData()\n",
    "        # Модель\n",
    "        self.model = modulus.pinn(cfg).to(self.device)\n",
    "\n",
    "        # Оптимизатор\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), betas=(0.999, 0.999), lr=self.learning_rate)\n",
    "\n",
    "        # Логирование\n",
    "        self.loss_history = []\n",
    "        self.l2_history = []\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    '''\n",
    "    Подключается к wandb для отслеживания процесса обучения\n",
    "    '''\n",
    "    def __wandbConnect(self, cfg):\n",
    "        wandb.login()\n",
    "        wandb.init(\n",
    "            project=cfg.project,\n",
    "            name=cfg.name,\n",
    "            config={\n",
    "            \"epochs\": cfg.epochs,\n",
    "            })\n",
    "    '''\n",
    "    Генерирует данные:\n",
    "      variables - выборка, содержищие граничные и начальные условия\n",
    "      variables_f - вся выборка\n",
    "      u_data - содержит решение для выборки variables\n",
    "    '''\n",
    "    def __createData(self):\n",
    "        self.t_data, self.x_data, self.u_data, self.t_data_f, self.x_data_f = dg.ac_generator(self.num_t, self.num_x)\n",
    "        self.variables = torch.FloatTensor(np.concatenate((self.t_data, self.x_data), axis=1)).to(self.device)\n",
    "        self.variables_f = torch.FloatTensor(np.concatenate((self.t_data_f, self.x_data_f), axis=1)).to(self.device)\n",
    "        self.variables_f.requires_grad = True\n",
    "        self.u_data = torch.FloatTensor(self.u_data).to(self.device)\n",
    "\n",
    "    '''\n",
    "    Считает L2 потерю относительно верного решения, находящимся по пути cfg.data_path\n",
    "    '''\n",
    "    def __calculate_l2_error(self):\n",
    "        t = np.linspace(0, 1, 201).reshape(-1, 1)\n",
    "        x = np.linspace(-1, 1, 513)[:-1].reshape(-1, 1)\n",
    "        T = t.shape[0]\n",
    "        N = x.shape[0]\n",
    "        T_star = np.tile(t, (1, N)).T\n",
    "        X_star = np.tile(x, (1, T))\n",
    "        t_test = T_star.flatten()[:, None]\n",
    "        x_test = X_star.flatten()[:, None]\n",
    "\n",
    "        test_variables = torch.FloatTensor(np.concatenate((t_test, x_test), axis=1)).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            u_pred = self.model(test_variables)\n",
    "        u_pred = u_pred.cpu().numpy().reshape(N, T)\n",
    "\n",
    "        # Сравнение с эталоном\n",
    "        data = scipy.io.loadmat(self.data_path)\n",
    "        exact_solution = np.real(data['uu'])\n",
    "        error = np.linalg.norm(u_pred - exact_solution, 2) / np.linalg.norm(exact_solution, 2)\n",
    "        return error\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Выводит график функции потерь, а также эпоху с наименьшей величиной потерь\n",
    "    '''\n",
    "    def printLossGraph(self):\n",
    "        print(f\"[Best][Epoch: {self.best_epoch}] Train loss: {self.best_loss}\")\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.l2_history)\n",
    "        plt.show()\n",
    "\n",
    "    '''\n",
    "    Выводит график вычисленного уравнения\n",
    "    '''\n",
    "    def printEval(self):\n",
    "      #Загружаем лучшие веса\n",
    "      self.model.load_state_dict(torch.load('./ac_1d.pth'))\n",
    "\n",
    "      #Подготовка данных и вывод L2\n",
    "      t = np.linspace(0, 1, 201).reshape(-1,1) # T x 1\n",
    "      x = np.linspace(-1, 1, 513)[:-1].reshape(-1,1) # N x 1\n",
    "      T = t.shape[0]\n",
    "      N = x.shape[0]\n",
    "      T_star = np.tile(t, (1, N)).T  # N x T\n",
    "      X_star = np.tile(x, (1, T))  # N x T\n",
    "      t_test = T_star.flatten()[:, None]\n",
    "      x_test = X_star.flatten()[:, None]\n",
    "\n",
    "      test_variables = torch.FloatTensor(np.concatenate((t_test, x_test), 1)).to(self.device)\n",
    "      with torch.no_grad():\n",
    "          u_pred = self.model(test_variables)\n",
    "      u_pred = u_pred.cpu().numpy().reshape(N,T)\n",
    "\n",
    "      data = scipy.io.loadmat(self.data_path)\n",
    "      Exact = np.real(data['uu'])\n",
    "      err = u_pred-Exact\n",
    "\n",
    "      err = np.linalg.norm(err,2)/np.linalg.norm(Exact,2)\n",
    "      print(f\"L2 Relative Error: {err}\")\n",
    "\n",
    "      #Рисуем графики\n",
    "      resplot(x, t, self.t_data, self.x_data, Exact, u_pred)\n",
    "\n",
    "      plt.figure(figsize=(10, 5))\n",
    "      plt.imshow(u_pred, interpolation='nearest', cmap='jet',\n",
    "                  extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "                  origin='lower', aspect='auto')\n",
    "      plt.clim(-1, 1)\n",
    "      plt.ylim(-1,1)\n",
    "      plt.xlim(0,1)\n",
    "      plt.scatter(self.t_data, self.x_data)\n",
    "      plt.xlabel('t')\n",
    "      plt.ylabel('x')\n",
    "      plt.title('u(t,x)')\n",
    "      plt.show()\n",
    "\n",
    "    '''\n",
    "    Функция тренировки нейросети\n",
    "    '''\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Предсказания\n",
    "            u_pred = self.model(self.variables)\n",
    "            print(u_pred)\n",
    "            exit()\n",
    "            u_pred_f = self.model(self.variables_f)\n",
    "\n",
    "            # Вычисление функции потерь\n",
    "            loss_f = torch.mean(ac_equation(u_pred_f, self.variables_f) ** 2)\n",
    "            loss_u = torch.mean((u_pred - self.u_data) ** 2)\n",
    "            loss = loss_f + loss_u\n",
    "\n",
    "            # Обновление весов\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            self.loss_history.append(current_loss)\n",
    "            l2_error = self.__calculate_l2_error()\n",
    "            self.l2_history.append(l2_error)\n",
    "\n",
    "            # Сохранение лучшей модели\n",
    "            if current_loss < self.best_loss:\n",
    "                self.best_loss = current_loss\n",
    "                self.best_epoch = epoch\n",
    "                torch.save(self.model.state_dict(), f'./ac_1d.pth')\n",
    "\n",
    "            # Логирование\n",
    "            if epoch:\n",
    "                print(f\"Epoch {epoch}, Train loss: {current_loss}, L2: {l2_error}\")\n",
    "                \n",
    "                if self.wandbFlag:\n",
    "                  wandb.log({\"epoche\": epoch, \"loss\": current_loss})\n",
    "                  wandb.log({\"epoche\": epoch, \"L2\": l2_error})\n",
    "\n",
    "            # if epoch % 500 == 0:\n",
    "            #     l2_error = self.__calculate_l2_error()\n",
    "            #     if self.wandbFlag:\n",
    "            #       wandb.log({\"epoche\": epoch, \"L2\": l2_error})\n",
    "            #     print(f\"L2 Relative Error: {l2_error}\")\n",
    "        self.printLossGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hnGE-gpF_mFN",
    "outputId": "4b2ba5ee-6d81-4403-b28f-d8900722eb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ml_collections.config_dict.config_dict.ConfigDict'>\n",
      "Operation mode:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/Calculation-module-based-on-PINN-neural-network-architecture/.venv/lib64/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1308],\n",
      "        [ 0.0861],\n",
      "        [ 0.0802],\n",
      "        [-0.1618],\n",
      "        [ 0.1059],\n",
      "        [-0.1777],\n",
      "        [-0.1738],\n",
      "        [ 0.1059],\n",
      "        [ 0.1130],\n",
      "        [ 0.0934],\n",
      "        [-0.1593],\n",
      "        [ 0.0940],\n",
      "        [ 0.0676],\n",
      "        [ 0.0654],\n",
      "        [-0.1663],\n",
      "        [ 0.1202],\n",
      "        [ 0.1263],\n",
      "        [-0.1606],\n",
      "        [-0.1103],\n",
      "        [-0.1247],\n",
      "        [ 0.1130],\n",
      "        [ 0.1243],\n",
      "        [ 0.0855],\n",
      "        [-0.1769],\n",
      "        [-0.1735],\n",
      "        [-0.1777],\n",
      "        [-0.1697],\n",
      "        [ 0.0855],\n",
      "        [-0.1411],\n",
      "        [ 0.0964],\n",
      "        [-0.1688],\n",
      "        [ 0.0710],\n",
      "        [-0.1688],\n",
      "        [-0.1697],\n",
      "        [-0.1746],\n",
      "        [ 0.1117],\n",
      "        [ 0.0977],\n",
      "        [ 0.1156],\n",
      "        [-0.1688],\n",
      "        [ 0.0767],\n",
      "        [-0.1560],\n",
      "        [-0.1288],\n",
      "        [-0.1766],\n",
      "        [-0.1393],\n",
      "        [-0.1103],\n",
      "        [-0.1641],\n",
      "        [ 0.1002],\n",
      "        [ 0.1065],\n",
      "        [ 0.0802],\n",
      "        [-0.1657],\n",
      "        [-0.1599],\n",
      "        [-0.1553],\n",
      "        [-0.1182],\n",
      "        [ 0.0855],\n",
      "        [-0.1437],\n",
      "        [-0.1204],\n",
      "        [ 0.1110],\n",
      "        [ 0.1156],\n",
      "        [ 0.0659],\n",
      "        [ 0.1136],\n",
      "        [-0.1580],\n",
      "        [ 0.0808],\n",
      "        [-0.1318],\n",
      "        [ 0.0750],\n",
      "        [-0.1560],\n",
      "        [-0.1618],\n",
      "        [ 0.1149],\n",
      "        [ 0.1110],\n",
      "        [ 0.0983],\n",
      "        [ 0.1236],\n",
      "        [ 0.0710],\n",
      "        [-0.1486],\n",
      "        [-0.1697],\n",
      "        [-0.1777],\n",
      "        [-0.1635],\n",
      "        [ 0.1256],\n",
      "        [-0.1356],\n",
      "        [-0.1318],\n",
      "        [ 0.1078],\n",
      "        [-0.1688],\n",
      "        [-0.1501],\n",
      "        [ 0.1052],\n",
      "        [-0.1715],\n",
      "        [-0.1783],\n",
      "        [ 0.1123],\n",
      "        [-0.1411],\n",
      "        [ 0.0843],\n",
      "        [-0.1327],\n",
      "        [ 0.0995],\n",
      "        [ 0.0773],\n",
      "        [ 0.0665],\n",
      "        [-0.1267],\n",
      "        [-0.1137],\n",
      "        [-0.1215],\n",
      "        [-0.1486],\n",
      "        [ 0.0710],\n",
      "        [-0.1762],\n",
      "        [-0.1486],\n",
      "        [ 0.0855],\n",
      "        [ 0.0682],\n",
      "        [-0.1785],\n",
      "        [ 0.0654],\n",
      "        [-0.0456],\n",
      "        [ 0.0175],\n",
      "        [ 0.0756],\n",
      "        [ 0.0690],\n",
      "        [-0.0130],\n",
      "        [-0.0579],\n",
      "        [ 0.0993],\n",
      "        [ 0.1231],\n",
      "        [-0.0732],\n",
      "        [-0.0504],\n",
      "        [-0.1033],\n",
      "        [-0.0741],\n",
      "        [-0.0715],\n",
      "        [-0.0784],\n",
      "        [-0.0965],\n",
      "        [-0.0369],\n",
      "        [-0.0868],\n",
      "        [-0.0008],\n",
      "        [ 0.1080],\n",
      "        [-0.0616],\n",
      "        [-0.0522],\n",
      "        [ 0.1140],\n",
      "        [ 0.0831],\n",
      "        [-0.0661],\n",
      "        [ 0.0812],\n",
      "        [-0.0818],\n",
      "        [ 0.1028],\n",
      "        [ 0.0195],\n",
      "        [ 0.0949],\n",
      "        [ 0.0206],\n",
      "        [-0.0349],\n",
      "        [ 0.0053],\n",
      "        [-0.0933],\n",
      "        [ 0.0165],\n",
      "        [-0.0995],\n",
      "        [-0.0925],\n",
      "        [ 0.1173],\n",
      "        [ 0.0526],\n",
      "        [ 0.0967],\n",
      "        [ 0.0565],\n",
      "        [ 0.0671],\n",
      "        [-0.0069],\n",
      "        [ 0.0437],\n",
      "        [ 0.0124],\n",
      "        [ 0.1223],\n",
      "        [ 0.1114],\n",
      "        [ 0.0397],\n",
      "        [-0.0980],\n",
      "        [ 0.0766],\n",
      "        [ 0.0940],\n",
      "        [ 0.0496],\n",
      "        [ 0.0226],\n",
      "        [ 0.0114],\n",
      "        [-0.0260],\n",
      "        [ 0.0145],\n",
      "        [ 0.0584],\n",
      "        [ 0.0913],\n",
      "        [ 0.0613],\n",
      "        [ 0.0246],\n",
      "        [ 0.1002],\n",
      "        [-0.0329],\n",
      "        [ 0.0367],\n",
      "        [-0.0827],\n",
      "        [ 0.0661],\n",
      "        [-0.0427],\n",
      "        [-0.0893],\n",
      "        [-0.0643],\n",
      "        [-0.0949],\n",
      "        [-0.0767],\n",
      "        [-0.0810],\n",
      "        [ 0.1123],\n",
      "        [-0.0569],\n",
      "        [-0.0319],\n",
      "        [ 0.0155],\n",
      "        [-0.1070],\n",
      "        [-0.0801],\n",
      "        [-0.0541],\n",
      "        [-0.0270],\n",
      "        [-0.0852],\n",
      "        [ 0.0545],\n",
      "        [-0.0290],\n",
      "        [ 0.1148],\n",
      "        [ 0.0216],\n",
      "        [-0.1018],\n",
      "        [-0.0038],\n",
      "        [ 0.0803],\n",
      "        [ 0.0043],\n",
      "        [ 0.0867],\n",
      "        [ 0.0236],\n",
      "        [ 0.1011],\n",
      "        [ 0.0407],\n",
      "        [-0.0670],\n",
      "        [ 0.0516],\n",
      "        [ 0.1247],\n",
      "        [ 0.0840],\n",
      "        [-0.0750],\n",
      "        [ 0.0535],\n",
      "        [-0.0901],\n",
      "        [ 0.1037],\n",
      "        [ 0.0719],\n",
      "        [-0.0724],\n",
      "        [ 0.0794],\n",
      "        [-0.0180],\n",
      "        [-0.0059],\n",
      "        [ 0.0023],\n",
      "        [-0.0099],\n",
      "        [-0.0706],\n",
      "        [-0.0532],\n",
      "        [-0.0089],\n",
      "        [-0.1026],\n",
      "        [ 0.0623],\n",
      "        [-0.0551],\n",
      "        [ 0.0457],\n",
      "        [ 0.0377],\n",
      "        [ 0.0594],\n",
      "        [ 0.1072],\n",
      "        [-0.0408],\n",
      "        [ 0.0002],\n",
      "        [-0.0240],\n",
      "        [ 0.1239],\n",
      "        [-0.0465],\n",
      "        [ 0.0555],\n",
      "        [ 0.1190],\n",
      "        [-0.0250],\n",
      "        [ 0.0858],\n",
      "        [-0.0909],\n",
      "        [-0.0339],\n",
      "        [-0.0697],\n",
      "        [-0.1048],\n",
      "        [-0.0475],\n",
      "        [-0.0776],\n",
      "        [-0.1063],\n",
      "        [-0.0280],\n",
      "        [-0.0835],\n",
      "        [-0.1003],\n",
      "        [ 0.0821],\n",
      "        [-0.0150],\n",
      "        [-0.0220],\n",
      "        [-0.0917],\n",
      "        [ 0.1165],\n",
      "        [-0.0606],\n",
      "        [ 0.0709],\n",
      "        [ 0.0931],\n",
      "        [-0.0941],\n",
      "        [ 0.1063],\n",
      "        [ 0.1131],\n",
      "        [ 0.0317],\n",
      "        [-0.0109],\n",
      "        [ 0.0427],\n",
      "        [-0.0988],\n",
      "        [ 0.0737],\n",
      "        [ 0.0975],\n",
      "        [ 0.0895],\n",
      "        [-0.0170],\n",
      "        [ 0.0347],\n",
      "        [ 0.0652],\n",
      "        [-0.0560],\n",
      "        [ 0.0256],\n",
      "        [ 0.1019],\n",
      "        [-0.0957],\n",
      "        [ 0.0984],\n",
      "        [-0.0378],\n",
      "        [-0.0588],\n",
      "        [-0.1077],\n",
      "        [ 0.0728],\n",
      "        [-0.0793],\n",
      "        [-0.0513],\n",
      "        [-0.1041],\n",
      "        [ 0.0747],\n",
      "        [-0.0679],\n",
      "        [ 0.1106],\n",
      "        [ 0.0700],\n",
      "        [-0.0359],\n",
      "        [-0.0140],\n",
      "        [-0.0388],\n",
      "        [ 0.0486],\n",
      "        [-0.0160],\n",
      "        [ 0.0467],\n",
      "        [ 0.0287],\n",
      "        [-0.0877],\n",
      "        [-0.0190],\n",
      "        [ 0.0104],\n",
      "        [-0.0119],\n",
      "        [-0.0028],\n",
      "        [-0.1011],\n",
      "        [ 0.1255],\n",
      "        [-0.0049],\n",
      "        [ 0.1181],\n",
      "        [-0.0844],\n",
      "        [-0.0494],\n",
      "        [ 0.0033],\n",
      "        [ 0.1214],\n",
      "        [ 0.0417],\n",
      "        [-0.0885],\n",
      "        [ 0.0307],\n",
      "        [ 0.0084],\n",
      "        [ 0.1198],\n",
      "        [ 0.1156],\n",
      "        [ 0.0849],\n",
      "        [ 0.0073],\n",
      "        [ 0.0877],\n",
      "        [-0.1055],\n",
      "        [-0.0200],\n",
      "        [-0.0652],\n",
      "        [-0.0398],\n",
      "        [ 0.1054],\n",
      "        [ 0.0185],\n",
      "        [ 0.0784],\n",
      "        [ 0.0633],\n",
      "        [-0.0310],\n",
      "        [ 0.1206],\n",
      "        [ 0.0904],\n",
      "        [-0.0079],\n",
      "        [ 0.0447],\n",
      "        [ 0.0094],\n",
      "        [-0.0634],\n",
      "        [-0.0688],\n",
      "        [ 0.1089],\n",
      "        [ 0.0357],\n",
      "        [-0.0758],\n",
      "        [-0.0597],\n",
      "        [-0.0972],\n",
      "        [-0.0485],\n",
      "        [ 0.0297],\n",
      "        [-0.1084],\n",
      "        [-0.0625],\n",
      "        [ 0.0642],\n",
      "        [ 0.0958],\n",
      "        [ 0.0775],\n",
      "        [ 0.0604],\n",
      "        [-0.0860],\n",
      "        [ 0.0063],\n",
      "        [-0.0300],\n",
      "        [ 0.0575],\n",
      "        [ 0.0134],\n",
      "        [ 0.0922],\n",
      "        [ 0.0476],\n",
      "        [-0.0437],\n",
      "        [ 0.0266],\n",
      "        [-0.0446],\n",
      "        [ 0.0276],\n",
      "        [-0.0210],\n",
      "        [ 0.0012],\n",
      "        [-0.0417],\n",
      "        [ 0.0681],\n",
      "        [ 0.0387],\n",
      "        [-0.0018],\n",
      "        [ 0.1263],\n",
      "        [ 0.0506],\n",
      "        [ 0.1097],\n",
      "        [ 0.0337],\n",
      "        [-0.0230]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:02<00:28,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2439],\n",
      "        [-0.1370],\n",
      "        [-0.1264],\n",
      "        [ 0.2554],\n",
      "        [-0.1658],\n",
      "        [ 0.2629],\n",
      "        [ 0.2609],\n",
      "        [-0.1658],\n",
      "        [-0.1735],\n",
      "        [-0.1488],\n",
      "        [ 0.2544],\n",
      "        [-0.1497],\n",
      "        [-0.1011],\n",
      "        [-0.0963],\n",
      "        [ 0.2574],\n",
      "        [-0.1800],\n",
      "        [-0.1844],\n",
      "        [ 0.2549],\n",
      "        [ 0.2371],\n",
      "        [ 0.2418],\n",
      "        [-0.1735],\n",
      "        [-0.1830],\n",
      "        [-0.1360],\n",
      "        [ 0.2625],\n",
      "        [ 0.2607],\n",
      "        [ 0.2629],\n",
      "        [ 0.2589],\n",
      "        [-0.1360],\n",
      "        [ 0.2475],\n",
      "        [-0.1534],\n",
      "        [ 0.2585],\n",
      "        [-0.1082],\n",
      "        [ 0.2585],\n",
      "        [ 0.2589],\n",
      "        [ 0.2613],\n",
      "        [-0.1722],\n",
      "        [-0.1551],\n",
      "        [-0.1760],\n",
      "        [ 0.2585],\n",
      "        [-0.1197],\n",
      "        [ 0.2531],\n",
      "        [ 0.2432],\n",
      "        [ 0.2623],\n",
      "        [ 0.2469],\n",
      "        [ 0.2371],\n",
      "        [ 0.2564],\n",
      "        [-0.1586],\n",
      "        [-0.1665],\n",
      "        [-0.1264],\n",
      "        [ 0.2571],\n",
      "        [ 0.2547],\n",
      "        [ 0.2528],\n",
      "        [ 0.2397],\n",
      "        [-0.1360],\n",
      "        [ 0.2484],\n",
      "        [ 0.2404],\n",
      "        [-0.1715],\n",
      "        [-0.1760],\n",
      "        [-0.0975],\n",
      "        [-0.1742],\n",
      "        [ 0.2539],\n",
      "        [-0.1275],\n",
      "        [ 0.2442],\n",
      "        [-0.1163],\n",
      "        [ 0.2531],\n",
      "        [ 0.2554],\n",
      "        [-0.1754],\n",
      "        [-0.1715],\n",
      "        [-0.1560],\n",
      "        [-0.1826],\n",
      "        [-0.1082],\n",
      "        [ 0.2503],\n",
      "        [ 0.2589],\n",
      "        [ 0.2629],\n",
      "        [ 0.2562],\n",
      "        [-0.1839],\n",
      "        [ 0.2456],\n",
      "        [ 0.2442],\n",
      "        [-0.1680],\n",
      "        [ 0.2585],\n",
      "        [ 0.2508],\n",
      "        [-0.1650],\n",
      "        [ 0.2597],\n",
      "        [ 0.2632],\n",
      "        [-0.1729],\n",
      "        [ 0.2475],\n",
      "        [-0.1339],\n",
      "        [ 0.2446],\n",
      "        [-0.1577],\n",
      "        [-0.1209],\n",
      "        [-0.0987],\n",
      "        [ 0.2425],\n",
      "        [ 0.2382],\n",
      "        [ 0.2408],\n",
      "        [ 0.2503],\n",
      "        [-0.1082],\n",
      "        [ 0.2621],\n",
      "        [ 0.2503],\n",
      "        [-0.1360],\n",
      "        [-0.1023],\n",
      "        [ 0.2634],\n",
      "        [-0.0963],\n",
      "        [ 0.1084],\n",
      "        [-0.0062],\n",
      "        [-0.1049],\n",
      "        [-0.0940],\n",
      "        [ 0.0481],\n",
      "        [ 0.1318],\n",
      "        [-0.1430],\n",
      "        [-0.1796],\n",
      "        [ 0.1619],\n",
      "        [ 0.1174],\n",
      "        [ 0.2241],\n",
      "        [ 0.1637],\n",
      "        [ 0.1584],\n",
      "        [ 0.1724],\n",
      "        [ 0.2095],\n",
      "        [ 0.0920],\n",
      "        [ 0.1894],\n",
      "        [ 0.0262],\n",
      "        [-0.1566],\n",
      "        [ 0.1390],\n",
      "        [ 0.1211],\n",
      "        [-0.1658],\n",
      "        [-0.1170],\n",
      "        [ 0.1479],\n",
      "        [-0.1140],\n",
      "        [ 0.1792],\n",
      "        [-0.1485],\n",
      "        [-0.0097],\n",
      "        [-0.1360],\n",
      "        [-0.0115],\n",
      "        [ 0.0884],\n",
      "        [ 0.0153],\n",
      "        [ 0.2029],\n",
      "        [-0.0044],\n",
      "        [ 0.2160],\n",
      "        [ 0.2012],\n",
      "        [-0.1709],\n",
      "        [-0.0665],\n",
      "        [-0.1388],\n",
      "        [-0.0731],\n",
      "        [-0.0908],\n",
      "        [ 0.0371],\n",
      "        [-0.0514],\n",
      "        [ 0.0028],\n",
      "        [-0.1783],\n",
      "        [-0.1619],\n",
      "        [-0.0446],\n",
      "        [ 0.2127],\n",
      "        [-0.1064],\n",
      "        [-0.1345],\n",
      "        [-0.0615],\n",
      "        [-0.0150],\n",
      "        [ 0.0045],\n",
      "        [ 0.0719],\n",
      "        [-0.0008],\n",
      "        [-0.0764],\n",
      "        [-0.1302],\n",
      "        [-0.0812],\n",
      "        [-0.0186],\n",
      "        [-0.1444],\n",
      "        [ 0.0847],\n",
      "        [-0.0395],\n",
      "        [ 0.1809],\n",
      "        [-0.0892],\n",
      "        [ 0.1029],\n",
      "        [ 0.1945],\n",
      "        [ 0.1443],\n",
      "        [ 0.2062],\n",
      "        [ 0.1689],\n",
      "        [ 0.1775],\n",
      "        [-0.1632],\n",
      "        [ 0.1301],\n",
      "        [ 0.0829],\n",
      "        [-0.0026],\n",
      "        [ 0.2320],\n",
      "        [ 0.1758],\n",
      "        [ 0.1247],\n",
      "        [ 0.0737],\n",
      "        [ 0.1861],\n",
      "        [-0.0698],\n",
      "        [ 0.0774],\n",
      "        [-0.1671],\n",
      "        [-0.0133],\n",
      "        [ 0.2209],\n",
      "        [ 0.0317],\n",
      "        [-0.1125],\n",
      "        [ 0.0172],\n",
      "        [-0.1230],\n",
      "        [-0.0168],\n",
      "        [-0.1458],\n",
      "        [-0.0463],\n",
      "        [ 0.1496],\n",
      "        [-0.0648],\n",
      "        [-0.1820],\n",
      "        [-0.1185],\n",
      "        [ 0.1654],\n",
      "        [-0.0682],\n",
      "        [ 0.1962],\n",
      "        [-0.1499],\n",
      "        [-0.0987],\n",
      "        [ 0.1602],\n",
      "        [-0.1110],\n",
      "        [ 0.0572],\n",
      "        [ 0.0353],\n",
      "        [ 0.0208],\n",
      "        [ 0.0426],\n",
      "        [ 0.1567],\n",
      "        [ 0.1229],\n",
      "        [ 0.0408],\n",
      "        [ 0.2225],\n",
      "        [-0.0828],\n",
      "        [ 0.1265],\n",
      "        [-0.0548],\n",
      "        [-0.0412],\n",
      "        [-0.0780],\n",
      "        [-0.1553],\n",
      "        [ 0.0993],\n",
      "        [ 0.0244],\n",
      "        [ 0.0682],\n",
      "        [-0.1808],\n",
      "        [ 0.1102],\n",
      "        [-0.0715],\n",
      "        [-0.1734],\n",
      "        [ 0.0701],\n",
      "        [-0.1215],\n",
      "        [ 0.1979],\n",
      "        [ 0.0865],\n",
      "        [ 0.1549],\n",
      "        [ 0.2273],\n",
      "        [ 0.1120],\n",
      "        [ 0.1706],\n",
      "        [ 0.2304],\n",
      "        [ 0.0755],\n",
      "        [ 0.1827],\n",
      "        [ 0.2176],\n",
      "        [-0.1155],\n",
      "        [ 0.0517],\n",
      "        [ 0.0646],\n",
      "        [ 0.1995],\n",
      "        [-0.1696],\n",
      "        [ 0.1372],\n",
      "        [-0.0971],\n",
      "        [-0.1331],\n",
      "        [ 0.2045],\n",
      "        [-0.1539],\n",
      "        [-0.1645],\n",
      "        [-0.0308],\n",
      "        [ 0.0444],\n",
      "        [-0.0497],\n",
      "        [ 0.2144],\n",
      "        [-0.1018],\n",
      "        [-0.1402],\n",
      "        [-0.1273],\n",
      "        [ 0.0554],\n",
      "        [-0.0360],\n",
      "        [-0.0876],\n",
      "        [ 0.1283],\n",
      "        [-0.0203],\n",
      "        [-0.1471],\n",
      "        [ 0.2078],\n",
      "        [-0.1416],\n",
      "        [ 0.0938],\n",
      "        [ 0.1336],\n",
      "        [ 0.2336],\n",
      "        [-0.1002],\n",
      "        [ 0.1741],\n",
      "        [ 0.1193],\n",
      "        [ 0.2257],\n",
      "        [-0.1033],\n",
      "        [ 0.1514],\n",
      "        [-0.1606],\n",
      "        [-0.0956],\n",
      "        [ 0.0902],\n",
      "        [ 0.0499],\n",
      "        [ 0.0957],\n",
      "        [-0.0598],\n",
      "        [ 0.0536],\n",
      "        [-0.0565],\n",
      "        [-0.0256],\n",
      "        [ 0.1911],\n",
      "        [ 0.0591],\n",
      "        [ 0.0063],\n",
      "        [ 0.0463],\n",
      "        [ 0.0298],\n",
      "        [ 0.2192],\n",
      "        [-0.1832],\n",
      "        [ 0.0335],\n",
      "        [-0.1721],\n",
      "        [ 0.1844],\n",
      "        [ 0.1156],\n",
      "        [ 0.0190],\n",
      "        [-0.1771],\n",
      "        [-0.0480],\n",
      "        [ 0.1928],\n",
      "        [-0.0291],\n",
      "        [ 0.0099],\n",
      "        [-0.1746],\n",
      "        [-0.1683],\n",
      "        [-0.1200],\n",
      "        [ 0.0117],\n",
      "        [-0.1244],\n",
      "        [ 0.2289],\n",
      "        [ 0.0609],\n",
      "        [ 0.1461],\n",
      "        [ 0.0975],\n",
      "        [-0.1526],\n",
      "        [-0.0080],\n",
      "        [-0.1095],\n",
      "        [-0.0844],\n",
      "        [ 0.0810],\n",
      "        [-0.1759],\n",
      "        [-0.1288],\n",
      "        [ 0.0390],\n",
      "        [-0.0531],\n",
      "        [ 0.0081],\n",
      "        [ 0.1425],\n",
      "        [ 0.1532],\n",
      "        [-0.1579],\n",
      "        [-0.0377],\n",
      "        [ 0.1672],\n",
      "        [ 0.1354],\n",
      "        [ 0.2111],\n",
      "        [ 0.1138],\n",
      "        [-0.0273],\n",
      "        [ 0.2352],\n",
      "        [ 0.1408],\n",
      "        [-0.0860],\n",
      "        [-0.1374],\n",
      "        [-0.1080],\n",
      "        [-0.0796],\n",
      "        [ 0.1878],\n",
      "        [ 0.0135],\n",
      "        [ 0.0792],\n",
      "        [-0.0747],\n",
      "        [ 0.0010],\n",
      "        [-0.1317],\n",
      "        [-0.0582],\n",
      "        [ 0.1048],\n",
      "        [-0.0221],\n",
      "        [ 0.1066],\n",
      "        [-0.0238],\n",
      "        [ 0.0627],\n",
      "        [ 0.0226],\n",
      "        [ 0.1011],\n",
      "        [-0.0924],\n",
      "        [-0.0429],\n",
      "        [ 0.0280],\n",
      "        [-0.1844],\n",
      "        [-0.0632],\n",
      "        [-0.1593],\n",
      "        [-0.0343],\n",
      "        [ 0.0664]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:05<00:25,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.8416199684143066, L2: 0.987773878941635\n",
      "tensor([[ 0.0523],\n",
      "        [-0.2499],\n",
      "        [-0.2383],\n",
      "        [ 0.0718],\n",
      "        [-0.2805],\n",
      "        [ 0.0923],\n",
      "        [ 0.0856],\n",
      "        [-0.2805],\n",
      "        [-0.2884],\n",
      "        [-0.2626],\n",
      "        [ 0.0697],\n",
      "        [-0.2636],\n",
      "        [-0.2104],\n",
      "        [-0.2050],\n",
      "        [ 0.0762],\n",
      "        [-0.2949],\n",
      "        [-0.2990],\n",
      "        [ 0.0707],\n",
      "        [ 0.0445],\n",
      "        [ 0.0497],\n",
      "        [-0.2884],\n",
      "        [-0.2977],\n",
      "        [-0.2488],\n",
      "        [ 0.0906],\n",
      "        [ 0.0851],\n",
      "        [ 0.0923],\n",
      "        [ 0.0801],\n",
      "        [-0.2488],\n",
      "        [ 0.0574],\n",
      "        [-0.2675],\n",
      "        [ 0.0789],\n",
      "        [-0.2183],\n",
      "        [ 0.0789],\n",
      "        [ 0.0801],\n",
      "        [ 0.0867],\n",
      "        [-0.2871],\n",
      "        [-0.2694],\n",
      "        [-0.2909],\n",
      "        [ 0.0789],\n",
      "        [-0.2310],\n",
      "        [ 0.0670],\n",
      "        [ 0.0515],\n",
      "        [ 0.0901],\n",
      "        [ 0.0565],\n",
      "        [ 0.0445],\n",
      "        [ 0.0740],\n",
      "        [-0.2730],\n",
      "        [-0.2813],\n",
      "        [-0.2383],\n",
      "        [ 0.0756],\n",
      "        [ 0.0702],\n",
      "        [ 0.0665],\n",
      "        [ 0.0472],\n",
      "        [-0.2488],\n",
      "        [ 0.0589],\n",
      "        [ 0.0480],\n",
      "        [-0.2864],\n",
      "        [-0.2909],\n",
      "        [-0.2064],\n",
      "        [-0.2891],\n",
      "        [ 0.0686],\n",
      "        [-0.2395],\n",
      "        [ 0.0528],\n",
      "        [-0.2273],\n",
      "        [ 0.0670],\n",
      "        [ 0.0718],\n",
      "        [-0.2903],\n",
      "        [-0.2864],\n",
      "        [-0.2703],\n",
      "        [-0.2973],\n",
      "        [-0.2183],\n",
      "        [ 0.0619],\n",
      "        [ 0.0801],\n",
      "        [ 0.0923],\n",
      "        [ 0.0734],\n",
      "        [-0.2986],\n",
      "        [ 0.0546],\n",
      "        [ 0.0528],\n",
      "        [-0.2828],\n",
      "        [ 0.0789],\n",
      "        [ 0.0629],\n",
      "        [-0.2797],\n",
      "        [ 0.0823],\n",
      "        [ 0.0934],\n",
      "        [-0.2878],\n",
      "        [ 0.0574],\n",
      "        [-0.2465],\n",
      "        [ 0.0532],\n",
      "        [-0.2721],\n",
      "        [-0.2322],\n",
      "        [-0.2077],\n",
      "        [ 0.0506],\n",
      "        [ 0.0456],\n",
      "        [ 0.0485],\n",
      "        [ 0.0619],\n",
      "        [-0.2183],\n",
      "        [ 0.0895],\n",
      "        [ 0.0619],\n",
      "        [-0.2488],\n",
      "        [-0.2117],\n",
      "        [ 0.0939],\n",
      "        [-0.2050],\n",
      "        [-0.0741],\n",
      "        [-0.1691],\n",
      "        [-0.2439],\n",
      "        [-0.2360],\n",
      "        [-0.1253],\n",
      "        [-0.0535],\n",
      "        [-0.2710],\n",
      "        [-0.2958],\n",
      "        [-0.0265],\n",
      "        [-0.0662],\n",
      "        [ 0.0318],\n",
      "        [-0.0249],\n",
      "        [-0.0296],\n",
      "        [-0.0169],\n",
      "        [ 0.0178],\n",
      "        [-0.0883],\n",
      "        [-0.0011],\n",
      "        [-0.1432],\n",
      "        [-0.2803],\n",
      "        [-0.0472],\n",
      "        [-0.0630],\n",
      "        [-0.2866],\n",
      "        [-0.2526],\n",
      "        [-0.0392],\n",
      "        [-0.2505],\n",
      "        [-0.0106],\n",
      "        [-0.2748],\n",
      "        [-0.1720],\n",
      "        [-0.2661],\n",
      "        [-0.1733],\n",
      "        [-0.0914],\n",
      "        [-0.1520],\n",
      "        [ 0.0115],\n",
      "        [-0.1677],\n",
      "        [ 0.0240],\n",
      "        [ 0.0100],\n",
      "        [-0.2900],\n",
      "        [-0.2156],\n",
      "        [-0.2680],\n",
      "        [-0.2206],\n",
      "        [-0.2337],\n",
      "        [-0.1343],\n",
      "        [-0.2042],\n",
      "        [-0.1621],\n",
      "        [-0.2950],\n",
      "        [-0.2839],\n",
      "        [-0.1990],\n",
      "        [ 0.0209],\n",
      "        [-0.2450],\n",
      "        [-0.2651],\n",
      "        [-0.2119],\n",
      "        [-0.1761],\n",
      "        [-0.1606],\n",
      "        [-0.1054],\n",
      "        [-0.1649],\n",
      "        [-0.2230],\n",
      "        [-0.2620],\n",
      "        [-0.2266],\n",
      "        [-0.1789],\n",
      "        [-0.2719],\n",
      "        [-0.0945],\n",
      "        [-0.1951],\n",
      "        [-0.0090],\n",
      "        [-0.2325],\n",
      "        [-0.0789],\n",
      "        [ 0.0037],\n",
      "        [-0.0424],\n",
      "        [ 0.0147],\n",
      "        [-0.0201],\n",
      "        [-0.0122],\n",
      "        [-0.2848],\n",
      "        [-0.0551],\n",
      "        [-0.0961],\n",
      "        [-0.1663],\n",
      "        [ 0.0395],\n",
      "        [-0.0137],\n",
      "        [-0.0599],\n",
      "        [-0.1038],\n",
      "        [-0.0042],\n",
      "        [-0.2181],\n",
      "        [-0.1007],\n",
      "        [-0.2874],\n",
      "        [-0.1747],\n",
      "        [ 0.0287],\n",
      "        [-0.1388],\n",
      "        [-0.2494],\n",
      "        [-0.1505],\n",
      "        [-0.2569],\n",
      "        [-0.1775],\n",
      "        [-0.2729],\n",
      "        [-0.2003],\n",
      "        [-0.0376],\n",
      "        [-0.2144],\n",
      "        [-0.2974],\n",
      "        [-0.2537],\n",
      "        [-0.0233],\n",
      "        [-0.2169],\n",
      "        [ 0.0052],\n",
      "        [-0.2757],\n",
      "        [-0.2394],\n",
      "        [-0.0280],\n",
      "        [-0.2483],\n",
      "        [-0.1177],\n",
      "        [-0.1358],\n",
      "        [-0.1476],\n",
      "        [-0.1298],\n",
      "        [-0.0312],\n",
      "        [-0.0615],\n",
      "        [-0.1313],\n",
      "        [ 0.0303],\n",
      "        [-0.2278],\n",
      "        [-0.0583],\n",
      "        [-0.2068],\n",
      "        [-0.1964],\n",
      "        [-0.2242],\n",
      "        [-0.2794],\n",
      "        [-0.0820],\n",
      "        [-0.1447],\n",
      "        [-0.1085],\n",
      "        [-0.2966],\n",
      "        [-0.0725],\n",
      "        [-0.2193],\n",
      "        [-0.2917],\n",
      "        [-0.1069],\n",
      "        [-0.2558],\n",
      "        [ 0.0068],\n",
      "        [-0.0930],\n",
      "        [-0.0328],\n",
      "        [ 0.0349],\n",
      "        [-0.0710],\n",
      "        [-0.0185],\n",
      "        [ 0.0380],\n",
      "        [-0.1023],\n",
      "        [-0.0074],\n",
      "        [ 0.0256],\n",
      "        [-0.2516],\n",
      "        [-0.1222],\n",
      "        [-0.1116],\n",
      "        [ 0.0084],\n",
      "        [-0.2891],\n",
      "        [-0.0487],\n",
      "        [-0.2383],\n",
      "        [-0.2641],\n",
      "        [ 0.0131],\n",
      "        [-0.2785],\n",
      "        [-0.2857],\n",
      "        [-0.1884],\n",
      "        [-0.1283],\n",
      "        [-0.2029],\n",
      "        [ 0.0225],\n",
      "        [-0.2417],\n",
      "        [-0.2690],\n",
      "        [-0.2600],\n",
      "        [-0.1192],\n",
      "        [-0.1924],\n",
      "        [-0.2313],\n",
      "        [-0.0567],\n",
      "        [-0.1803],\n",
      "        [-0.2738],\n",
      "        [ 0.0162],\n",
      "        [-0.2700],\n",
      "        [-0.0867],\n",
      "        [-0.0519],\n",
      "        [ 0.0410],\n",
      "        [-0.2405],\n",
      "        [-0.0153],\n",
      "        [-0.0646],\n",
      "        [ 0.0333],\n",
      "        [-0.2428],\n",
      "        [-0.0360],\n",
      "        [-0.2830],\n",
      "        [-0.2371],\n",
      "        [-0.0898],\n",
      "        [-0.1238],\n",
      "        [-0.0851],\n",
      "        [-0.2106],\n",
      "        [-0.1207],\n",
      "        [-0.2081],\n",
      "        [-0.1844],\n",
      "        [ 0.0005],\n",
      "        [-0.1161],\n",
      "        [-0.1592],\n",
      "        [-0.1268],\n",
      "        [-0.1402],\n",
      "        [ 0.0272],\n",
      "        [-0.2982],\n",
      "        [-0.1373],\n",
      "        [-0.2908],\n",
      "        [-0.0058],\n",
      "        [-0.0678],\n",
      "        [-0.1491],\n",
      "        [-0.2942],\n",
      "        [-0.2016],\n",
      "        [ 0.0021],\n",
      "        [-0.1871],\n",
      "        [-0.1563],\n",
      "        [-0.2925],\n",
      "        [-0.2883],\n",
      "        [-0.2548],\n",
      "        [-0.1549],\n",
      "        [-0.2579],\n",
      "        [ 0.0364],\n",
      "        [-0.1146],\n",
      "        [-0.0408],\n",
      "        [-0.0836],\n",
      "        [-0.2776],\n",
      "        [-0.1706],\n",
      "        [-0.2472],\n",
      "        [-0.2290],\n",
      "        [-0.0976],\n",
      "        [-0.2933],\n",
      "        [-0.2610],\n",
      "        [-0.1328],\n",
      "        [-0.2055],\n",
      "        [-0.1578],\n",
      "        [-0.0440],\n",
      "        [-0.0344],\n",
      "        [-0.2812],\n",
      "        [-0.1938],\n",
      "        [-0.0217],\n",
      "        [-0.0503],\n",
      "        [ 0.0194],\n",
      "        [-0.0694],\n",
      "        [-0.1857],\n",
      "        [ 0.0426],\n",
      "        [-0.0456],\n",
      "        [-0.2302],\n",
      "        [-0.2670],\n",
      "        [-0.2461],\n",
      "        [-0.2254],\n",
      "        [-0.0026],\n",
      "        [-0.1534],\n",
      "        [-0.0992],\n",
      "        [-0.2218],\n",
      "        [-0.1635],\n",
      "        [-0.2630],\n",
      "        [-0.2093],\n",
      "        [-0.0773],\n",
      "        [-0.1816],\n",
      "        [-0.0757],\n",
      "        [-0.1830],\n",
      "        [-0.1131],\n",
      "        [-0.1461],\n",
      "        [-0.0804],\n",
      "        [-0.2348],\n",
      "        [-0.1977],\n",
      "        [-0.1417],\n",
      "        [-0.2990],\n",
      "        [-0.2131],\n",
      "        [-0.2821],\n",
      "        [-0.1911],\n",
      "        [-0.1100]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:07<00:19,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.9525290727615356, L2: 1.006698425849849\n",
      "tensor([[ 0.0443],\n",
      "        [-0.0403],\n",
      "        [-0.0312],\n",
      "        [ 0.0640],\n",
      "        [-0.0666],\n",
      "        [ 0.0856],\n",
      "        [ 0.0785],\n",
      "        [-0.0666],\n",
      "        [-0.0744],\n",
      "        [-0.0508],\n",
      "        [ 0.0618],\n",
      "        [-0.0516],\n",
      "        [-0.0104],\n",
      "        [-0.0065],\n",
      "        [ 0.0685],\n",
      "        [-0.0813],\n",
      "        [-0.0864],\n",
      "        [ 0.0629],\n",
      "        [ 0.0367],\n",
      "        [ 0.0418],\n",
      "        [-0.0744],\n",
      "        [-0.0848],\n",
      "        [-0.0394],\n",
      "        [ 0.0838],\n",
      "        [ 0.0779],\n",
      "        [ 0.0856],\n",
      "        [ 0.0726],\n",
      "        [-0.0394],\n",
      "        [ 0.0494],\n",
      "        [-0.0549],\n",
      "        [ 0.0714],\n",
      "        [-0.0161],\n",
      "        [ 0.0714],\n",
      "        [ 0.0726],\n",
      "        [ 0.0796],\n",
      "        [-0.0730],\n",
      "        [-0.0565],\n",
      "        [-0.0770],\n",
      "        [ 0.0714],\n",
      "        [-0.0256],\n",
      "        [ 0.0591],\n",
      "        [ 0.0435],\n",
      "        [ 0.0832],\n",
      "        [ 0.0484],\n",
      "        [ 0.0367],\n",
      "        [ 0.0663],\n",
      "        [-0.0597],\n",
      "        [-0.0674],\n",
      "        [-0.0312],\n",
      "        [ 0.0680],\n",
      "        [ 0.0623],\n",
      "        [ 0.0585],\n",
      "        [ 0.0394],\n",
      "        [-0.0394],\n",
      "        [ 0.0508],\n",
      "        [ 0.0402],\n",
      "        [-0.0723],\n",
      "        [-0.0770],\n",
      "        [-0.0075],\n",
      "        [-0.0750],\n",
      "        [ 0.0607],\n",
      "        [-0.0322],\n",
      "        [ 0.0448],\n",
      "        [-0.0228],\n",
      "        [ 0.0591],\n",
      "        [ 0.0640],\n",
      "        [-0.0763],\n",
      "        [-0.0723],\n",
      "        [-0.0574],\n",
      "        [-0.0842],\n",
      "        [-0.0161],\n",
      "        [ 0.0538],\n",
      "        [ 0.0726],\n",
      "        [ 0.0856],\n",
      "        [ 0.0657],\n",
      "        [-0.0858],\n",
      "        [ 0.0466],\n",
      "        [ 0.0448],\n",
      "        [-0.0688],\n",
      "        [ 0.0714],\n",
      "        [ 0.0549],\n",
      "        [-0.0659],\n",
      "        [ 0.0749],\n",
      "        [ 0.0868],\n",
      "        [-0.0737],\n",
      "        [ 0.0494],\n",
      "        [-0.0376],\n",
      "        [ 0.0452],\n",
      "        [-0.0590],\n",
      "        [-0.0266],\n",
      "        [-0.0084],\n",
      "        [ 0.0426],\n",
      "        [ 0.0378],\n",
      "        [ 0.0406],\n",
      "        [ 0.0538],\n",
      "        [-0.0161],\n",
      "        [ 0.0826],\n",
      "        [ 0.0538],\n",
      "        [-0.0394],\n",
      "        [-0.0113],\n",
      "        [ 0.0874],\n",
      "        [-0.0065],\n",
      "        [-0.0085],\n",
      "        [-0.0423],\n",
      "        [-0.0681],\n",
      "        [-0.0654],\n",
      "        [-0.0269],\n",
      "        [-0.0009],\n",
      "        [-0.0772],\n",
      "        [-0.0854],\n",
      "        [ 0.0092],\n",
      "        [-0.0056],\n",
      "        [ 0.0315],\n",
      "        [ 0.0098],\n",
      "        [ 0.0080],\n",
      "        [ 0.0128],\n",
      "        [ 0.0261],\n",
      "        [-0.0136],\n",
      "        [ 0.0188],\n",
      "        [-0.0332],\n",
      "        [-0.0803],\n",
      "        [ 0.0015],\n",
      "        [-0.0044],\n",
      "        [-0.0824],\n",
      "        [-0.0711],\n",
      "        [ 0.0044],\n",
      "        [-0.0703],\n",
      "        [ 0.0152],\n",
      "        [-0.0785],\n",
      "        [-0.0433],\n",
      "        [-0.0756],\n",
      "        [-0.0438],\n",
      "        [-0.0147],\n",
      "        [-0.0363],\n",
      "        [ 0.0236],\n",
      "        [-0.0418],\n",
      "        [ 0.0285],\n",
      "        [ 0.0230],\n",
      "        [-0.0835],\n",
      "        [-0.0584],\n",
      "        [-0.0762],\n",
      "        [-0.0601],\n",
      "        [-0.0646],\n",
      "        [-0.0301],\n",
      "        [-0.0545],\n",
      "        [-0.0399],\n",
      "        [-0.0851],\n",
      "        [-0.0815],\n",
      "        [-0.0527],\n",
      "        [ 0.0273],\n",
      "        [-0.0685],\n",
      "        [-0.0752],\n",
      "        [-0.0572],\n",
      "        [-0.0448],\n",
      "        [-0.0394],\n",
      "        [-0.0198],\n",
      "        [-0.0409],\n",
      "        [-0.0610],\n",
      "        [-0.0742],\n",
      "        [-0.0622],\n",
      "        [-0.0457],\n",
      "        [-0.0775],\n",
      "        [-0.0159],\n",
      "        [-0.0514],\n",
      "        [ 0.0158],\n",
      "        [-0.0642],\n",
      "        [-0.0102],\n",
      "        [ 0.0206],\n",
      "        [ 0.0032],\n",
      "        [ 0.0249],\n",
      "        [ 0.0116],\n",
      "        [ 0.0146],\n",
      "        [-0.0818],\n",
      "        [-0.0015],\n",
      "        [-0.0164],\n",
      "        [-0.0414],\n",
      "        [ 0.0346],\n",
      "        [ 0.0140],\n",
      "        [-0.0032],\n",
      "        [-0.0192],\n",
      "        [ 0.0176],\n",
      "        [-0.0593],\n",
      "        [-0.0181],\n",
      "        [-0.0826],\n",
      "        [-0.0443],\n",
      "        [ 0.0303],\n",
      "        [-0.0317],\n",
      "        [-0.0700],\n",
      "        [-0.0358],\n",
      "        [-0.0725],\n",
      "        [-0.0453],\n",
      "        [-0.0778],\n",
      "        [-0.0532],\n",
      "        [ 0.0050],\n",
      "        [-0.0580],\n",
      "        [-0.0859],\n",
      "        [-0.0714],\n",
      "        [ 0.0104],\n",
      "        [-0.0589],\n",
      "        [ 0.0212],\n",
      "        [-0.0788],\n",
      "        [-0.0666],\n",
      "        [ 0.0086],\n",
      "        [-0.0696],\n",
      "        [-0.0242],\n",
      "        [-0.0306],\n",
      "        [-0.0348],\n",
      "        [-0.0285],\n",
      "        [ 0.0074],\n",
      "        [-0.0038],\n",
      "        [-0.0290],\n",
      "        [ 0.0309],\n",
      "        [-0.0626],\n",
      "        [-0.0026],\n",
      "        [-0.0554],\n",
      "        [-0.0518],\n",
      "        [-0.0614],\n",
      "        [-0.0800],\n",
      "        [-0.0113],\n",
      "        [-0.0337],\n",
      "        [-0.0209],\n",
      "        [-0.0856],\n",
      "        [-0.0079],\n",
      "        [-0.0597],\n",
      "        [-0.0840],\n",
      "        [-0.0203],\n",
      "        [-0.0721],\n",
      "        [ 0.0218],\n",
      "        [-0.0153],\n",
      "        [ 0.0068],\n",
      "        [ 0.0327],\n",
      "        [-0.0073],\n",
      "        [ 0.0122],\n",
      "        [ 0.0340],\n",
      "        [-0.0187],\n",
      "        [ 0.0164],\n",
      "        [ 0.0291],\n",
      "        [-0.0707],\n",
      "        [-0.0258],\n",
      "        [-0.0220],\n",
      "        [ 0.0224],\n",
      "        [-0.0832],\n",
      "        [ 0.0009],\n",
      "        [-0.0662],\n",
      "        [-0.0749],\n",
      "        [ 0.0243],\n",
      "        [-0.0797],\n",
      "        [-0.0821],\n",
      "        [-0.0491],\n",
      "        [-0.0279],\n",
      "        [-0.0541],\n",
      "        [ 0.0279],\n",
      "        [-0.0673],\n",
      "        [-0.0766],\n",
      "        [-0.0735],\n",
      "        [-0.0247],\n",
      "        [-0.0504],\n",
      "        [-0.0638],\n",
      "        [-0.0021],\n",
      "        [-0.0462],\n",
      "        [-0.0782],\n",
      "        [ 0.0255],\n",
      "        [-0.0769],\n",
      "        [-0.0130],\n",
      "        [-0.0003],\n",
      "        [ 0.0352],\n",
      "        [-0.0670],\n",
      "        [ 0.0134],\n",
      "        [-0.0050],\n",
      "        [ 0.0321],\n",
      "        [-0.0677],\n",
      "        [ 0.0056],\n",
      "        [-0.0812],\n",
      "        [-0.0658],\n",
      "        [-0.0142],\n",
      "        [-0.0263],\n",
      "        [-0.0125],\n",
      "        [-0.0567],\n",
      "        [-0.0252],\n",
      "        [-0.0558],\n",
      "        [-0.0476],\n",
      "        [ 0.0194],\n",
      "        [-0.0236],\n",
      "        [-0.0389],\n",
      "        [-0.0274],\n",
      "        [-0.0322],\n",
      "        [ 0.0297],\n",
      "        [-0.0861],\n",
      "        [-0.0311],\n",
      "        [-0.0838],\n",
      "        [ 0.0170],\n",
      "        [-0.0061],\n",
      "        [-0.0353],\n",
      "        [-0.0848],\n",
      "        [-0.0536],\n",
      "        [ 0.0200],\n",
      "        [-0.0486],\n",
      "        [-0.0378],\n",
      "        [-0.0843],\n",
      "        [-0.0829],\n",
      "        [-0.0718],\n",
      "        [-0.0373],\n",
      "        [-0.0728],\n",
      "        [ 0.0334],\n",
      "        [-0.0231],\n",
      "        [ 0.0038],\n",
      "        [-0.0119],\n",
      "        [-0.0794],\n",
      "        [-0.0428],\n",
      "        [-0.0692],\n",
      "        [-0.0630],\n",
      "        [-0.0170],\n",
      "        [-0.0846],\n",
      "        [-0.0739],\n",
      "        [-0.0295],\n",
      "        [-0.0550],\n",
      "        [-0.0383],\n",
      "        [ 0.0026],\n",
      "        [ 0.0062],\n",
      "        [-0.0806],\n",
      "        [-0.0509],\n",
      "        [ 0.0110],\n",
      "        [ 0.0003],\n",
      "        [ 0.0267],\n",
      "        [-0.0067],\n",
      "        [-0.0481],\n",
      "        [ 0.0358],\n",
      "        [ 0.0021],\n",
      "        [-0.0634],\n",
      "        [-0.0759],\n",
      "        [-0.0689],\n",
      "        [-0.0618],\n",
      "        [ 0.0182],\n",
      "        [-0.0368],\n",
      "        [-0.0175],\n",
      "        [-0.0606],\n",
      "        [-0.0404],\n",
      "        [-0.0746],\n",
      "        [-0.0563],\n",
      "        [-0.0096],\n",
      "        [-0.0467],\n",
      "        [-0.0090],\n",
      "        [-0.0472],\n",
      "        [-0.0225],\n",
      "        [-0.0343],\n",
      "        [-0.0107],\n",
      "        [-0.0650],\n",
      "        [-0.0523],\n",
      "        [-0.0327],\n",
      "        [-0.0864],\n",
      "        [-0.0576],\n",
      "        [-0.0809],\n",
      "        [-0.0500],\n",
      "        [-0.0214]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:09<00:16,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.43405577540397644, L2: 1.0431985229622254\n",
      "tensor([[0.0274],\n",
      "        [0.1378],\n",
      "        [0.1436],\n",
      "        [0.0433],\n",
      "        [0.1189],\n",
      "        [0.0627],\n",
      "        [0.0561],\n",
      "        [0.1189],\n",
      "        [0.1125],\n",
      "        [0.1307],\n",
      "        [0.0414],\n",
      "        [0.1301],\n",
      "        [0.1560],\n",
      "        [0.1581],\n",
      "        [0.0472],\n",
      "        [0.1063],\n",
      "        [0.1014],\n",
      "        [0.0423],\n",
      "        [0.0221],\n",
      "        [0.0255],\n",
      "        [0.1125],\n",
      "        [0.1031],\n",
      "        [0.1384],\n",
      "        [0.0611],\n",
      "        [0.0556],\n",
      "        [0.0627],\n",
      "        [0.0508],\n",
      "        [0.1384],\n",
      "        [0.0312],\n",
      "        [0.1277],\n",
      "        [0.0498],\n",
      "        [0.1527],\n",
      "        [0.0498],\n",
      "        [0.0508],\n",
      "        [0.0572],\n",
      "        [0.1137],\n",
      "        [0.1265],\n",
      "        [0.1103],\n",
      "        [0.0498],\n",
      "        [0.1470],\n",
      "        [0.0391],\n",
      "        [0.0267],\n",
      "        [0.0605],\n",
      "        [0.0305],\n",
      "        [0.0221],\n",
      "        [0.0452],\n",
      "        [0.1242],\n",
      "        [0.1183],\n",
      "        [0.1436],\n",
      "        [0.0467],\n",
      "        [0.0418],\n",
      "        [0.0386],\n",
      "        [0.0239],\n",
      "        [0.1384],\n",
      "        [0.0324],\n",
      "        [0.0244],\n",
      "        [0.1142],\n",
      "        [0.1103],\n",
      "        [0.1576],\n",
      "        [0.1120],\n",
      "        [0.0404],\n",
      "        [0.1430],\n",
      "        [0.0277],\n",
      "        [0.1488],\n",
      "        [0.0391],\n",
      "        [0.0433],\n",
      "        [0.1108],\n",
      "        [0.1142],\n",
      "        [0.1259],\n",
      "        [0.1036],\n",
      "        [0.1527],\n",
      "        [0.0348],\n",
      "        [0.0508],\n",
      "        [0.0627],\n",
      "        [0.0447],\n",
      "        [0.1020],\n",
      "        [0.0291],\n",
      "        [0.0277],\n",
      "        [0.1171],\n",
      "        [0.0498],\n",
      "        [0.0356],\n",
      "        [0.1195],\n",
      "        [0.0529],\n",
      "        [0.0638],\n",
      "        [0.1131],\n",
      "        [0.0312],\n",
      "        [0.1395],\n",
      "        [0.0280],\n",
      "        [0.1248],\n",
      "        [0.1465],\n",
      "        [0.1570],\n",
      "        [0.0261],\n",
      "        [0.0228],\n",
      "        [0.0247],\n",
      "        [0.0348],\n",
      "        [0.1527],\n",
      "        [0.0600],\n",
      "        [0.0348],\n",
      "        [0.1384],\n",
      "        [0.1554],\n",
      "        [0.0644],\n",
      "        [0.1581],\n",
      "        [0.0434],\n",
      "        [0.0653],\n",
      "        [0.0849],\n",
      "        [0.0827],\n",
      "        [0.0547],\n",
      "        [0.0391],\n",
      "        [0.0927],\n",
      "        [0.1004],\n",
      "        [0.0338],\n",
      "        [0.0417],\n",
      "        [0.0238],\n",
      "        [0.0335],\n",
      "        [0.0344],\n",
      "        [0.0320],\n",
      "        [0.0260],\n",
      "        [0.0464],\n",
      "        [0.0292],\n",
      "        [0.0589],\n",
      "        [0.0955],\n",
      "        [0.0378],\n",
      "        [0.0410],\n",
      "        [0.0975],\n",
      "        [0.0874],\n",
      "        [0.0363],\n",
      "        [0.0868],\n",
      "        [0.0309],\n",
      "        [0.0939],\n",
      "        [0.0660],\n",
      "        [0.0913],\n",
      "        [0.0663],\n",
      "        [0.0471],\n",
      "        [0.0610],\n",
      "        [0.0270],\n",
      "        [0.0649],\n",
      "        [0.0250],\n",
      "        [0.0273],\n",
      "        [0.0986],\n",
      "        [0.0772],\n",
      "        [0.0918],\n",
      "        [0.0785],\n",
      "        [0.0821],\n",
      "        [0.0568],\n",
      "        [0.0742],\n",
      "        [0.0635],\n",
      "        [0.1001],\n",
      "        [0.0967],\n",
      "        [0.0728],\n",
      "        [0.0255],\n",
      "        [0.0852],\n",
      "        [0.0910],\n",
      "        [0.0762],\n",
      "        [0.0670],\n",
      "        [0.0632],\n",
      "        [0.0502],\n",
      "        [0.0642],\n",
      "        [0.0792],\n",
      "        [0.0901],\n",
      "        [0.0802],\n",
      "        [0.0677],\n",
      "        [0.0930],\n",
      "        [0.0478],\n",
      "        [0.0718],\n",
      "        [0.0306],\n",
      "        [0.0818],\n",
      "        [0.0444],\n",
      "        [0.0284],\n",
      "        [0.0369],\n",
      "        [0.0265],\n",
      "        [0.0326],\n",
      "        [0.0312],\n",
      "        [0.0969],\n",
      "        [0.0394],\n",
      "        [0.0481],\n",
      "        [0.0646],\n",
      "        [0.0226],\n",
      "        [0.0315],\n",
      "        [0.0404],\n",
      "        [0.0498],\n",
      "        [0.0298],\n",
      "        [0.0779],\n",
      "        [0.0491],\n",
      "        [0.0977],\n",
      "        [0.0667],\n",
      "        [0.0243],\n",
      "        [0.0579],\n",
      "        [0.0865],\n",
      "        [0.0607],\n",
      "        [0.0886],\n",
      "        [0.0674],\n",
      "        [0.0933],\n",
      "        [0.0732],\n",
      "        [0.0360],\n",
      "        [0.0769],\n",
      "        [0.1009],\n",
      "        [0.0877],\n",
      "        [0.0332],\n",
      "        [0.0775],\n",
      "        [0.0281],\n",
      "        [0.0941],\n",
      "        [0.0837],\n",
      "        [0.0341],\n",
      "        [0.0862],\n",
      "        [0.0530],\n",
      "        [0.0572],\n",
      "        [0.0600],\n",
      "        [0.0558],\n",
      "        [0.0347],\n",
      "        [0.0407],\n",
      "        [0.0561],\n",
      "        [0.0240],\n",
      "        [0.0805],\n",
      "        [0.0401],\n",
      "        [0.0749],\n",
      "        [0.0722],\n",
      "        [0.0795],\n",
      "        [0.0953],\n",
      "        [0.0450],\n",
      "        [0.0593],\n",
      "        [0.0509],\n",
      "        [0.1007],\n",
      "        [0.0430],\n",
      "        [0.0782],\n",
      "        [0.0991],\n",
      "        [0.0505],\n",
      "        [0.0883],\n",
      "        [0.0278],\n",
      "        [0.0474],\n",
      "        [0.0350],\n",
      "        [0.0233],\n",
      "        [0.0427],\n",
      "        [0.0323],\n",
      "        [0.0228],\n",
      "        [0.0495],\n",
      "        [0.0303],\n",
      "        [0.0248],\n",
      "        [0.0871],\n",
      "        [0.0540],\n",
      "        [0.0516],\n",
      "        [0.0276],\n",
      "        [0.0983],\n",
      "        [0.0381],\n",
      "        [0.0834],\n",
      "        [0.0907],\n",
      "        [0.0268],\n",
      "        [0.0950],\n",
      "        [0.0972],\n",
      "        [0.0701],\n",
      "        [0.0554],\n",
      "        [0.0739],\n",
      "        [0.0252],\n",
      "        [0.0843],\n",
      "        [0.0921],\n",
      "        [0.0895],\n",
      "        [0.0533],\n",
      "        [0.0711],\n",
      "        [0.0814],\n",
      "        [0.0397],\n",
      "        [0.0680],\n",
      "        [0.0936],\n",
      "        [0.0263],\n",
      "        [0.0924],\n",
      "        [0.0461],\n",
      "        [0.0388],\n",
      "        [0.0224],\n",
      "        [0.0840],\n",
      "        [0.0318],\n",
      "        [0.0414],\n",
      "        [0.0235],\n",
      "        [0.0846],\n",
      "        [0.0356],\n",
      "        [0.0964],\n",
      "        [0.0830],\n",
      "        [0.0467],\n",
      "        [0.0544],\n",
      "        [0.0457],\n",
      "        [0.0759],\n",
      "        [0.0537],\n",
      "        [0.0752],\n",
      "        [0.0691],\n",
      "        [0.0289],\n",
      "        [0.0526],\n",
      "        [0.0628],\n",
      "        [0.0551],\n",
      "        [0.0582],\n",
      "        [0.0245],\n",
      "        [0.1012],\n",
      "        [0.0575],\n",
      "        [0.0988],\n",
      "        [0.0300],\n",
      "        [0.0420],\n",
      "        [0.0603],\n",
      "        [0.0999],\n",
      "        [0.0735],\n",
      "        [0.0287],\n",
      "        [0.0698],\n",
      "        [0.0621],\n",
      "        [0.0994],\n",
      "        [0.0980],\n",
      "        [0.0880],\n",
      "        [0.0618],\n",
      "        [0.0889],\n",
      "        [0.0231],\n",
      "        [0.0523],\n",
      "        [0.0366],\n",
      "        [0.0454],\n",
      "        [0.0947],\n",
      "        [0.0656],\n",
      "        [0.0859],\n",
      "        [0.0808],\n",
      "        [0.0484],\n",
      "        [0.0996],\n",
      "        [0.0898],\n",
      "        [0.0565],\n",
      "        [0.0745],\n",
      "        [0.0625],\n",
      "        [0.0372],\n",
      "        [0.0353],\n",
      "        [0.0958],\n",
      "        [0.0715],\n",
      "        [0.0329],\n",
      "        [0.0385],\n",
      "        [0.0258],\n",
      "        [0.0424],\n",
      "        [0.0694],\n",
      "        [0.0221],\n",
      "        [0.0375],\n",
      "        [0.0811],\n",
      "        [0.0916],\n",
      "        [0.0855],\n",
      "        [0.0798],\n",
      "        [0.0295],\n",
      "        [0.0614],\n",
      "        [0.0488],\n",
      "        [0.0789],\n",
      "        [0.0639],\n",
      "        [0.0904],\n",
      "        [0.0755],\n",
      "        [0.0440],\n",
      "        [0.0684],\n",
      "        [0.0437],\n",
      "        [0.0687],\n",
      "        [0.0519],\n",
      "        [0.0596],\n",
      "        [0.0447],\n",
      "        [0.0824],\n",
      "        [0.0725],\n",
      "        [0.0586],\n",
      "        [0.1014],\n",
      "        [0.0765],\n",
      "        [0.0961],\n",
      "        [0.0708],\n",
      "        [0.0512]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:11<00:12,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.6094091534614563, L2: 1.0328487933471815\n",
      "tensor([[-4.7225e-02],\n",
      "        [ 1.6946e-01],\n",
      "        [ 1.7150e-01],\n",
      "        [-4.1494e-02],\n",
      "        [ 1.6203e-01],\n",
      "        [-2.8175e-02],\n",
      "        [-3.3125e-02],\n",
      "        [ 1.6203e-01],\n",
      "        [ 1.5928e-01],\n",
      "        [ 1.6681e-01],\n",
      "        [-4.2531e-02],\n",
      "        [ 1.6658e-01],\n",
      "        [ 1.7534e-01],\n",
      "        [ 1.7592e-01],\n",
      "        [-3.9151e-02],\n",
      "        [ 1.5650e-01],\n",
      "        [ 1.5424e-01],\n",
      "        [-4.2024e-02],\n",
      "        [-4.6203e-02],\n",
      "        [-4.7179e-02],\n",
      "        [ 1.5928e-01],\n",
      "        [ 1.5499e-01],\n",
      "        [ 1.6967e-01],\n",
      "        [-2.9458e-02],\n",
      "        [-3.3514e-02],\n",
      "        [-2.8175e-02],\n",
      "        [-3.6830e-02],\n",
      "        [ 1.6967e-01],\n",
      "        [-4.6645e-02],\n",
      "        [ 1.6565e-01],\n",
      "        [-3.7517e-02],\n",
      "        [ 1.7439e-01],\n",
      "        [-3.7517e-02],\n",
      "        [-3.6830e-02],\n",
      "        [-3.2335e-02],\n",
      "        [ 1.5978e-01],\n",
      "        [ 1.6518e-01],\n",
      "        [ 1.5827e-01],\n",
      "        [-3.7517e-02],\n",
      "        [ 1.7264e-01],\n",
      "        [-4.3693e-02],\n",
      "        [-4.7238e-02],\n",
      "        [-2.9879e-02],\n",
      "        [-4.6814e-02],\n",
      "        [-4.6203e-02],\n",
      "        [-4.0366e-02],\n",
      "        [ 1.6423e-01],\n",
      "        [ 1.6178e-01],\n",
      "        [ 1.7150e-01],\n",
      "        [-3.9463e-02],\n",
      "        [-4.2280e-02],\n",
      "        [-4.3907e-02],\n",
      "        [-4.6876e-02],\n",
      "        [ 1.6967e-01],\n",
      "        [-4.6340e-02],\n",
      "        [-4.7006e-02],\n",
      "        [ 1.6004e-01],\n",
      "        [ 1.5827e-01],\n",
      "        [ 1.7578e-01],\n",
      "        [ 1.5903e-01],\n",
      "        [-4.3014e-02],\n",
      "        [ 1.7130e-01],\n",
      "        [-4.7207e-02],\n",
      "        [ 1.7319e-01],\n",
      "        [-4.3693e-02],\n",
      "        [-4.1494e-02],\n",
      "        [ 1.5852e-01],\n",
      "        [ 1.6004e-01],\n",
      "        [ 1.6494e-01],\n",
      "        [ 1.5524e-01],\n",
      "        [ 1.7439e-01],\n",
      "        [-4.5546e-02],\n",
      "        [-3.6830e-02],\n",
      "        [-2.8175e-02],\n",
      "        [-4.0656e-02],\n",
      "        [ 1.5449e-01],\n",
      "        [-4.7067e-02],\n",
      "        [-4.7207e-02],\n",
      "        [ 1.6129e-01],\n",
      "        [-3.7517e-02],\n",
      "        [-4.5227e-02],\n",
      "        [ 1.6228e-01],\n",
      "        [-3.5399e-02],\n",
      "        [-2.7305e-02],\n",
      "        [ 1.5953e-01],\n",
      "        [-4.6645e-02],\n",
      "        [ 1.7009e-01],\n",
      "        [-4.7183e-02],\n",
      "        [ 1.6447e-01],\n",
      "        [ 1.7246e-01],\n",
      "        [ 1.7563e-01],\n",
      "        [-4.7222e-02],\n",
      "        [-4.6534e-02],\n",
      "        [-4.7059e-02],\n",
      "        [-4.5546e-02],\n",
      "        [ 1.7439e-01],\n",
      "        [-3.0297e-02],\n",
      "        [-4.5546e-02],\n",
      "        [ 1.6967e-01],\n",
      "        [ 1.7518e-01],\n",
      "        [-2.6866e-02],\n",
      "        [ 1.7592e-01],\n",
      "        [ 9.8730e-03],\n",
      "        [ 6.4185e-02],\n",
      "        [ 1.1303e-01],\n",
      "        [ 1.0754e-01],\n",
      "        [ 3.8107e-02],\n",
      "        [-8.4051e-04],\n",
      "        [ 1.3249e-01],\n",
      "        [ 1.5166e-01],\n",
      "        [-1.4316e-02],\n",
      "        [ 5.7160e-03],\n",
      "        [-4.0906e-02],\n",
      "        [-1.5086e-02],\n",
      "        [-1.2766e-02],\n",
      "        [-1.8901e-02],\n",
      "        [-3.4829e-02],\n",
      "        [ 1.7454e-02],\n",
      "        [-2.6321e-02],\n",
      "        [ 4.8551e-02],\n",
      "        [ 1.3955e-01],\n",
      "        [-4.0704e-03],\n",
      "        [ 4.0655e-03],\n",
      "        [ 1.4435e-01],\n",
      "        [ 1.1918e-01],\n",
      "        [-8.0583e-03],\n",
      "        [ 1.1765e-01],\n",
      "        [-2.1903e-02],\n",
      "        [ 1.3534e-01],\n",
      "        [ 6.5912e-02],\n",
      "        [ 1.2887e-01],\n",
      "        [ 6.6774e-02],\n",
      "        [ 1.9154e-02],\n",
      "        [ 5.3774e-02],\n",
      "        [-3.2043e-02],\n",
      "        [ 6.3320e-02],\n",
      "        [-3.7563e-02],\n",
      "        [-3.1338e-02],\n",
      "        [ 1.4704e-01],\n",
      "        [ 9.3808e-02],\n",
      "        [ 1.3032e-01],\n",
      "        [ 9.7085e-02],\n",
      "        [ 1.0595e-01],\n",
      "        [ 4.3326e-02],\n",
      "        [ 8.6340e-02],\n",
      "        [ 5.9855e-02],\n",
      "        [ 1.5100e-01],\n",
      "        [ 1.4231e-01],\n",
      "        [ 8.2982e-02],\n",
      "        [-3.6202e-02],\n",
      "        [ 1.1380e-01],\n",
      "        [ 1.2813e-01],\n",
      "        [ 9.1332e-02],\n",
      "        [ 6.8497e-02],\n",
      "        [ 5.8988e-02],\n",
      "        [ 2.6859e-02],\n",
      "        [ 6.1589e-02],\n",
      "        [ 9.8713e-02],\n",
      "        [ 1.2593e-01],\n",
      "        [ 1.0114e-01],\n",
      "        [ 7.0217e-02],\n",
      "        [ 1.3320e-01],\n",
      "        [ 2.0859e-02],\n",
      "        [ 8.0450e-02],\n",
      "        [-2.2647e-02],\n",
      "        [ 1.0515e-01],\n",
      "        [ 1.2387e-02],\n",
      "        [-2.8490e-02],\n",
      "        [-6.4700e-03],\n",
      "        [-3.3442e-02],\n",
      "        [-1.7383e-02],\n",
      "        [-2.1157e-02],\n",
      "        [ 1.4299e-01],\n",
      "        [-2.7824e-05],\n",
      "        [ 2.1713e-02],\n",
      "        [ 6.2455e-02],\n",
      "        [-4.4165e-02],\n",
      "        [-2.0408e-02],\n",
      "        [ 2.4223e-03],\n",
      "        [ 2.5999e-02],\n",
      "        [-2.4860e-02],\n",
      "        [ 9.5450e-02],\n",
      "        [ 2.4282e-02],\n",
      "        [ 1.4503e-01],\n",
      "        [ 6.7636e-02],\n",
      "        [-3.9579e-02],\n",
      "        [ 4.5938e-02],\n",
      "        [ 1.1689e-01],\n",
      "        [ 5.2904e-02],\n",
      "        [ 1.2220e-01],\n",
      "        [ 6.9358e-02],\n",
      "        [ 1.3392e-01],\n",
      "        [ 8.3824e-02],\n",
      "        [-8.8490e-03],\n",
      "        [ 9.2984e-02],\n",
      "        [ 1.5295e-01],\n",
      "        [ 1.1994e-01],\n",
      "        [-1.5855e-02],\n",
      "        [ 9.4630e-02],\n",
      "        [-2.9206e-02],\n",
      "        [ 1.3605e-01],\n",
      "        [ 1.0990e-01],\n",
      "        [-1.3542e-02],\n",
      "        [ 1.1612e-01],\n",
      "        [ 3.3769e-02],\n",
      "        [ 4.4196e-02],\n",
      "        [ 5.1163e-02],\n",
      "        [ 4.0715e-02],\n",
      "        [-1.1988e-02],\n",
      "        [ 3.2429e-03],\n",
      "        [ 4.1585e-02],\n",
      "        [-4.0244e-02],\n",
      "        [ 1.0195e-01],\n",
      "        [ 1.6036e-03],\n",
      "        [ 8.8010e-02],\n",
      "        [ 8.1295e-02],\n",
      "        [ 9.9525e-02],\n",
      "        [ 1.3886e-01],\n",
      "        [ 1.4070e-02],\n",
      "        [ 4.9422e-02],\n",
      "        [ 2.8582e-02],\n",
      "        [ 1.5231e-01],\n",
      "        [ 9.0383e-03],\n",
      "        [ 9.6268e-02],\n",
      "        [ 1.4837e-01],\n",
      "        [ 2.7720e-02],\n",
      "        [ 1.2145e-01],\n",
      "        [-2.9920e-02],\n",
      "        [ 2.0006e-02],\n",
      "        [-1.1207e-02],\n",
      "        [-4.2220e-02],\n",
      "        [ 8.2052e-03],\n",
      "        [-1.8143e-02],\n",
      "        [-4.3520e-02],\n",
      "        [ 2.5140e-02],\n",
      "        [-2.3387e-02],\n",
      "        [-3.8238e-02],\n",
      "        [ 1.1842e-01],\n",
      "        [ 3.6370e-02],\n",
      "        [ 3.0308e-02],\n",
      "        [-3.0631e-02],\n",
      "        [ 1.4637e-01],\n",
      "        [-3.2662e-03],\n",
      "        [ 1.0912e-01],\n",
      "        [ 1.2740e-01],\n",
      "        [-3.2744e-02],\n",
      "        [ 1.3816e-01],\n",
      "        [ 1.4368e-01],\n",
      "        [ 7.6205e-02],\n",
      "        [ 3.9845e-02],\n",
      "        [ 8.5502e-02],\n",
      "        [-3.6884e-02],\n",
      "        [ 1.1147e-01],\n",
      "        [ 1.3105e-01],\n",
      "        [ 1.2444e-01],\n",
      "        [ 3.4635e-02],\n",
      "        [ 7.8756e-02],\n",
      "        [ 1.0435e-01],\n",
      "        [ 7.8690e-04],\n",
      "        [ 7.1075e-02],\n",
      "        [ 1.3463e-01],\n",
      "        [-3.4137e-02],\n",
      "        [ 1.3177e-01],\n",
      "        [ 1.6606e-02],\n",
      "        [-1.6512e-03],\n",
      "        [-4.4806e-02],\n",
      "        [ 1.1069e-01],\n",
      "        [-1.9656e-02],\n",
      "        [ 4.8898e-03],\n",
      "        [-4.1565e-02],\n",
      "        [ 1.1225e-01],\n",
      "        [-9.6373e-03],\n",
      "        [ 1.4162e-01],\n",
      "        [ 1.0833e-01],\n",
      "        [ 1.8304e-02],\n",
      "        [ 3.7238e-02],\n",
      "        [ 1.5760e-02],\n",
      "        [ 9.0504e-02],\n",
      "        [ 3.5503e-02],\n",
      "        [ 8.8843e-02],\n",
      "        [ 7.3645e-02],\n",
      "        [-2.7047e-02],\n",
      "        [ 3.2902e-02],\n",
      "        [ 5.8120e-02],\n",
      "        [ 3.8976e-02],\n",
      "        [ 4.6809e-02],\n",
      "        [-3.8910e-02],\n",
      "        [ 1.5360e-01],\n",
      "        [ 4.5067e-02],\n",
      "        [ 1.4771e-01],\n",
      "        [-2.4125e-02],\n",
      "        [ 6.5440e-03],\n",
      "        [ 5.2034e-02],\n",
      "        [ 1.5035e-01],\n",
      "        [ 8.4664e-02],\n",
      "        [-2.7770e-02],\n",
      "        [ 7.5353e-02],\n",
      "        [ 5.6382e-02],\n",
      "        [ 1.4904e-01],\n",
      "        [ 1.4570e-01],\n",
      "        [ 1.2069e-01],\n",
      "        [ 5.5513e-02],\n",
      "        [ 1.2295e-01],\n",
      "        [-4.2871e-02],\n",
      "        [ 3.2037e-02],\n",
      "        [-7.2654e-03],\n",
      "        [ 1.4914e-02],\n",
      "        [ 1.3746e-01],\n",
      "        [ 6.5049e-02],\n",
      "        [ 1.1535e-01],\n",
      "        [ 1.0275e-01],\n",
      "        [ 2.2568e-02],\n",
      "        [ 1.4969e-01],\n",
      "        [ 1.2518e-01],\n",
      "        [ 4.2455e-02],\n",
      "        [ 8.7176e-02],\n",
      "        [ 5.7251e-02],\n",
      "        [-5.6724e-03],\n",
      "        [-1.0423e-02],\n",
      "        [ 1.4025e-01],\n",
      "        [ 7.9603e-02],\n",
      "        [-1.6620e-02],\n",
      "        [-2.4597e-03],\n",
      "        [-3.5517e-02],\n",
      "        [ 7.3738e-03],\n",
      "        [ 7.4499e-02],\n",
      "        [-4.5444e-02],\n",
      "        [-4.8725e-03],\n",
      "        [ 1.0355e-01],\n",
      "        [ 1.2959e-01],\n",
      "        [ 1.1458e-01],\n",
      "        [ 1.0033e-01],\n",
      "        [-2.5592e-02],\n",
      "        [ 5.4644e-02],\n",
      "        [ 2.3424e-02],\n",
      "        [ 9.7900e-02],\n",
      "        [ 6.0722e-02],\n",
      "        [ 1.2666e-01],\n",
      "        [ 8.9674e-02],\n",
      "        [ 1.1547e-02],\n",
      "        [ 7.1933e-02],\n",
      "        [ 1.0709e-02],\n",
      "        [ 7.2789e-02],\n",
      "        [ 3.1172e-02],\n",
      "        [ 5.0292e-02],\n",
      "        [ 1.3228e-02],\n",
      "        [ 1.0674e-01],\n",
      "        [ 8.2139e-02],\n",
      "        [ 4.7680e-02],\n",
      "        [ 1.5424e-01],\n",
      "        [ 9.2159e-02],\n",
      "        [ 1.4094e-01],\n",
      "        [ 7.7907e-02],\n",
      "        [ 2.9445e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:13<00:15,  2.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m TrainClass(default\u001b[38;5;241m.\u001b[39mget_config())\n\u001b[0;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# a.printLossGraph()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m a\u001b[38;5;241m.\u001b[39mprintEval()\n",
      "Cell \u001b[0;32mIn[7], line 167\u001b[0m, in \u001b[0;36mTrainClass.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_history\u001b[38;5;241m.\u001b[39mappend(current_loss)\n\u001b[0;32m--> 167\u001b[0m l2_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__calculate_l2_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_history\u001b[38;5;241m.\u001b[39mappend(l2_error)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Сохранение лучшей модели\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mTrainClass.__calculate_l2_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mloadmat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[1;32m     79\u001b[0m exact_solution \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreal(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muu\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 80\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexact_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(exact_solution, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error\n",
      "File \u001b[0;32m~/Calculation-module-based-on-PINN-neural-network-architecture/.venv/lib64/python3.12/site-packages/numpy/linalg/_linalg.py:2851\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuplicate axes given.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2851\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_multi_svd_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2853\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _multi_svd_norm(x, row_axis, col_axis, amin)\n",
      "File \u001b[0;32m~/Calculation-module-based-on-PINN-neural-network-architecture/.venv/lib64/python3.12/site-packages/numpy/linalg/_linalg.py:2616\u001b[0m, in \u001b[0;36m_multi_svd_norm\u001b[0;34m(x, row_axis, col_axis, op)\u001b[0m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute a function of the singular values of the 2-D matrices in `x`.\u001b[39;00m\n\u001b[1;32m   2594\u001b[0m \n\u001b[1;32m   2595\u001b[0m \u001b[38;5;124;03mThis is a private utility function used by `numpy.linalg.norm()`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2613\u001b[0m \n\u001b[1;32m   2614\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m y \u001b[38;5;241m=\u001b[39m moveaxis(x, (row_axis, col_axis), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m-> 2616\u001b[0m result \u001b[38;5;241m=\u001b[39m op(\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Calculation-module-based-on-PINN-neural-network-architecture/.venv/lib64/python3.12/site-packages/numpy/linalg/_linalg.py:1849\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1845\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->d\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_svd_nonconvergence,\n\u001b[1;32m   1847\u001b[0m               invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1848\u001b[0m               under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 1849\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1850\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = TrainClass(default.get_config())\n",
    "a.train()\n",
    "# a.printLossGraph()\n",
    "a.printEval()\n",
    "# a.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
